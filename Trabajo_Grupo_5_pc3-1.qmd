---
title: "Visualizaci√≥n de datos"
author: "ASTRYD"
format: html
editor: visual
Integrantes del grupo 5:
- Mantilla Saravia Daniel Jos√©
- Pachas Ventura Luis Marco
- Mendoza Felipa Astryd Xihomara
- Marcos Avalos Ruth Lizeth Edith
- Pachas Munayco Walter Manuel
- Acero Valencia Rodrigo
---

# Paquetes para visualizar datos

```{r}

install.packages("gridExtra")
install.packages("gtsummary")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("flextable")

```

```{r}
library(tidyverse)
library(rio)
library(here)
library(gridExtra) ## Para m√∫ltiple gr√°ficos en una sola p√°gina
library(GGally) ## Para gr√°ficos de correlaci√≥n
library(forcats)
library(gtsummary)
library(dplyr)
library(ggplot2)
library(flextable)
```

# Cargando los datos

El *dataset* contiene datos de 218 pacientes con c√°ncer cervical. El *dataset* incluye 18 variables entre √©tnicas, ocupacionales y variables ocupacionales.

```{r}
data_cerv_0 <- import(here("data", "conoc_actit_factor_cancer_cervical.csv"))
```

## Examinamos los datos

`str()` es la funci√≥n para ver la estructura de los datos.

```{r}
str(data_cerv_0)
```

```{r}
install.packages("skimr")
```

```{r}
library("skimr")
```

```{r}
skim(data_cerv)
```

## Conversi√≥n de caracteres a factor (categ√≥ricos) usando la funci√≥n `mutate_if()`

Las variables categ√≥ricas (ej. Estadio T) han sido importadas como caracteres. Necesitamos transformalo a factores. En RStudio, factores es el tipo de dato para trabajar con variables categ√≥ricas.

```{r}
data_cerv <- data_cerv_0 |> 
  mutate_if(is.character, as.factor)
str(data_cerv)
```

# Visualizando datos: el molde

Para realizar visualizaciones con el paquete ggplot2, debemos reemplazar lo que esta encerrado en los signos. Este es el molde fundamental para crear gr√°ficos m√°s complejos.

`<midata> |> ggplot(aes(x = <var1>, y = <var2>)) + geom_<xxxx>()`

-   <midata> : el nombre del dataset a utilizar.
-   \|\> : esto es llamado "pipe", la cual conecta los datos a la funci√≥n ggplot
-   \+ : usa + par conectar declaraciones de ggplot
-   <var> : la variable, cuyos datos ser√°n usados para crear el gr√°fico.
-   geom\_<xxxx>: indica la funci√≥n para crear el tipo de gr√°fico. Ej. geom_bar, para crear gr√°ficos de barra.

# Visualizando distribuci√≥n de datos

# 1. Visualizando datos categ√≥ricos

Gr√°ficos de barra Los gr√°ficos de barra son adecuados para mostrar frecuencias de variables categ√≥ricas.

```{r}
data_cerv |>  
  ggplot(aes(x = etnia)) +
  geom_bar()
```

Aqu√≠, a√±adimos la funci√≥n `fct_infreq()` de paquete forcats para ordenar (en orden decreciente) las barras del conteo, por estado marital.

```{r}
data_cerv |>  
  ggplot(aes(x = fct_infreq(etnia))) +
  geom_bar()
```

Con la funci√≥n `labs()` podemos a√±adir nombres a los ejes del gr√°ficos.

```{r}
data_cerv |>  
  ggplot(aes(x = fct_infreq(etnia))) +
  geom_bar() +
  labs(y = "Frecuencia", x = "etnia")
```

Para el gr√°fico de barra podemos usar frecuencias relativas. Por ejemplo, un gr√°fico de barras que muestre proporciones. Aqu√≠ es necesario calcular las proporciones. Nota que seguida a `y =` se muestra el c√°lculo para convertir los conteos a proporciones.

```{r}
data_cerv |>  
  ggplot(aes(x = etnia, y = ..count../sum(after_stat(count)))) +
  geom_bar() +
  labs(y = "Porcentaje", x = "etnia")
```

# 2. Visualizando Datos Num√©ricos

## 2.1. Con histogramas

Para visualizar conteos. Nota que aqu√≠, la variable `edad` es num√©rica y la funci√≥n para producir un histograma es `geom_histogram()`

```{r}
data_cerv |>  
  ggplot(aes(x = edad)) +
  geom_histogram() +
  labs(y = "Frecuencia", x = "edad")
```

Un histograma de proporciones. Aqu√≠ `..density..` es la estimaci√≥n de densidad que reemplaza al conteo crudo. Toda el area del gr√°fico de densidad suma 1.

```{r}
data_cerv  |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(aes(y = ..density..)) +
  labs(y = "Density", x = "num_hijos")
```

A veces, puede ser util visualizar gr√°ficos de lado a lado. Aqu√≠ dos histogramas lado a lado usando la funci√≥n `grid.arrange()`

```{r}
hist_1 = data_cerv |> ggplot(aes(x = num_hijos)) +
  geom_histogram() +
  labs(y = "Frecuencia", x = "num_hijos")

hist_2 = data_cerv  |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(aes(y = ..density..)) +
  labs(y = "Density", x = "num_hijos")
```

```{r}
grid.arrange(hist_1, hist_2, ncol = 2)
```

Conteo con un n√∫mero de barras distinto

Podemos cambiar los intervalos para la generaci√≥n del histograma usando el argumento bins dentro de la funci√≥n `geom_histogram()`

```{r}
data_cerv |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(bins = 30) +
  labs(y = "Frecuencia", x = "num_hijos")
```

Modificando los colores de las barras del histograma.

```{r}
data_cerv |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(
    color = "black", ## Color de las barras
    fill = "green" ## Color de las barras
    ) + 
  labs(y = "Frecuencia", 
       x = "num_hijos")
```

Modificando color en gr√°ficos de barras. Nota que aqu√≠, usamos el argumento fill para colorear las barras pertenecientes a las categor√≠as.

```{r}
data_cerv |>  
  ggplot(aes(x = fct_infreq(etnia), fill = etnia)) +
  geom_bar() +
  labs(y = "Frecuencia", x = "etnia")
```

## 2.2. Con Boxplots (gr√°fico de cajas y bigotes)

Para mostrar datos de una variable en un gr√°fico de cajas y bigotes usamos la funci√≥n `geom_boxplot()`

```{r}
data_mama |> 
  ggplot(aes(y = Albumina_g_dL)) + ## Cambia y por x para invertir el gr√°fico
  geom_boxplot() +
  theme(axis.text.x  = element_blank(),
        axis.ticks.x = element_blank()) +
  labs(y = "Albumina")
```

La funci√≥n nativa de R, `boxplot()`, permite realizar el mismo gr√°fico.

```{r}
box_album_base = boxplot(data_mama$Albumina_g_dL,
                         ylab = "Albumina",
                         horizontal = TRUE, ## Cambia la direcci√≥n del gr√°fico
                         col = "salmon") ## A√±adimos color
  
```

# 3. Visualizando variables categ√≥ricas *versus* categ√≥ricas

```{r}
data_cerv |> 
  ggplot(aes(x = etnia, fill = ocupacion)) +
  geom_bar(position = "dodge") + ## Sin este argumento, las barras estar√°n una sobre otras
  labs(y = "Frecuencia",  
       x = "etnia",
       fill = "ocupacion")
```

Qu√© esta mal con esto?

```{r}
data_mama |>   
  group_by(Estadio_T, Estado_des)  |>  
  count() |>   
  # Compute proportions within grade
  # n is the default variable created by count()
  group_by(Estadio_T) |>   
  mutate(Proportion = n/sum(n))  |> 
  ggplot(aes(x = Estadio_T, y = Proportion, fill = Estado_des)) +
  geom_bar(position = 'dodge', stat = 'identity') +
  labs(y = "Proportion",
       x = "Estadio T",
       fill = "Desenlace")
```

```{r}
addmargins(prop.table(
  table(data_mama$Estado_des, data_mama$Estadio_T), 
  margin = 2), 1)
```

# 4. Visualizando distribuci√≥n de variables continuas *versus* categ√≥ricas

## 4.1. Gr√°ficos de barras

```{r}
data_mama |> 
  filter(!is.na(Recep_estrogeno) & !is.na(Estadio_T)) |> 
  group_by(Recep_estrogeno, Estadio_T) |> 
  summarise(n = n(),
            promedio = mean(Ki67_express, na.rm = T),
            de = sd(Ki67_express, na.rm = T)) |> 
  ggplot(aes(x = Recep_estrogeno, y = promedio, fill = Estadio_T)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = promedio - de, ymax = promedio + de),
                width = 0.5, size = 0.75, position = position_dodge(0.9)) +
  labs(y = "Expresi√≥n de KI67", fill = "Estadio_T", x = "Recep_estrogeno")
```

## 4.1. Boxplots lado a lado

```{r}
data_mama |>  
  filter(!is.na(Tam_tumor)& !is.na(Estadio_T))  |> 
  ggplot(aes(y = Tam_tumor, x = Estadio_T)) +
  geom_boxplot() +
  labs(y = "Tama√±o", x = "Estadio T")
```

```{r}
data_mama |>  
  filter(!is.na(Tam_tumor)& !is.na(Estadio_T) & !is.na(Recep_estrogeno))  |> 
  ggplot(aes(y = Tam_tumor, x = Estadio_T, fill = Recep_estrogeno)) +
  geom_boxplot() +
  labs(y = "Tama√±o", x = "Estadio T")
```

```{r}
data_mama |>  
  filter(!is.na(Tam_tumor)& !is.na(Estadio_T) & !is.na(Recep_estrogeno))  |> 
  ggplot(aes(y = Tam_tumor, x = Recep_estrogeno, fill = Estadio_T)) +
  geom_boxplot() +
  labs(y = "Tama√±o", x = "Receptor")
```

## 4.3. Filas de histogramas

```{r}
data_mama  |>  
  filter(!is.na(Estadio_T) & !is.na(Estado_des) & !is.na(hemoglobina_g_dL)) |>
  group_by(Estadio_N) |>  
  ggplot(aes(x = hemoglobina_g_dL)) +
  geom_histogram(aes(y = ..density..), bins = 20,
                 color = "black", fill = "white") +
  labs(x = "Hemoglobina (mg/dL)", y = "Proporci√≥n") +
  facet_wrap(~Estadio_N, nrow = 4) +
  ggtitle("Hemoglobina por Estadio N")
```

# 5. Visualizaci√≥n para variables continuas versus continuas

Usamos la funci√≥n geom_point para generar gr√°ficos de dispersi√≥n y visualizar la relaci√≥n de dos varaibles num√©ricas

```{r}
data_mama |> 
  ggplot(aes(x = Albumina_g_dL, y = hemoglobina_g_dL)) +
  geom_point() +
  labs(x = "Albumina (mg/dL)", y = "Hemoglobina (mg/dL)")
```

La funci√≥n geom_smoth a√±ade una l√≠nea de regresi√≥n al gr√°fico. "lm" es para linear model

```{r}
data_mama |> 
  ggplot(aes(x = Albumina_g_dL, y = hemoglobina_g_dL)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Albumina (mg/dL)", y = "Hemoglobina (mg/dL)")
```

Aqu√≠, funci√≥n geom_smooth() a√±ade una l√≠nea de tendencia suavizada al gr√°fico.

```{r}
data_mama |> 
  ggplot(aes(x = Albumina_g_dL, y = hemoglobina_g_dL)) +
  geom_point() +
  geom_smooth(stat = "smooth", se = TRUE) +
  labs(x = "Albumina (mg/dL)", y = "Hemoglobina (mg/dL)")
```

Finalmente, la funci√≥n `ggpairs()` permite visualizar multiple variables numerica a la vez. Aqu√≠, combinamos la funci√≥nm select() (para seleccionar las variables num√©ricas) y ggpairs, para generar el gr√°fico y los coeficientes de correlaci√≥n.

```{r}
data_mama |>  
  select(hemoglobina_g_dL, Albumina_g_dL, Supervivencia_meses) %>% 
  ggpairs(columnLabels = c("Hemoglobina", "Albumin", "Supervivencia meses"))
```

# 6. Exportando figuras

```{r}
ggsave(
  "nombre_de_objeto.png",
  width = 7, # Ancho
  height = 4, # Alto
  dpi = 300 # resoluci√≥n
) 
```

# 7. Prueba de Ajuste

**1. Modelo estad√≠stico**\
Es una representaci√≥n matem√°tica que describe relaciones entre variables. Se utiliza para hacer inferencias, predicciones o entender el comportamiento de los datos.

**2. Variable continua**\
Tipo de variable num√©rica que puede tomar un n√∫mero infinito de valores dentro de un rango (por ejemplo, niveles de glucosa o circunferencia de cintura).

**3. Distribuci√≥n de probabilidad**\
Describe c√≥mo se distribuyen los valores posibles de una variable aleatoria. Algunas distribuciones comunes incluyen la normal, binomial y chi-cuadrado.

**4. Hip√≥tesis nula (H‚ÇÄ)**\
Proposici√≥n que se plantea al inicio de una prueba estad√≠stica y que indica la ausencia de efecto o diferencia. Se rechaza o no seg√∫n el valor p obtenido.

**5. Valor p**\
Probabilidad de obtener un resultado igual o m√°s extremo que el observado, si la hip√≥tesis nula fuera cierta. Un valor p menor a 0.05 suele considerarse estad√≠sticamente significativo.

# ¬øQue son las pruebas de bondad de ajuste?

Las pruebas de bondad de ajust**e** eval√∫an qu√© tan bien los datos observados se ajustan a los valores esperados seg√∫n un modelo estad√≠stico.

La bondad de ajuste puede evaluarse en al menos dos escenarios principales:

### 1. En modelos de regresi√≥n

Por ejemplo, un estudiante podr√≠a aplicar un modelo de regresi√≥n lineal para evaluar la relaci√≥n entre el peso de los pacientes de un hospital y su nivel de glucosa. Para determinar si el modelo es adecuado para explicar esta relaci√≥n, se puede calcular el estad√≠stico de bondad de ajuste R¬≤.

El estad√≠stico R¬≤ mide el porcentaje de variabilidad de la variable dependiente (en este caso, el nivel de glucosa) que es explicado por el modelo de regresi√≥n. Cuanto mayor sea el valor de R¬≤, mejor ser√° el ajuste del modelo a los datos observados.

### 2. En distribuciones de probabilidad

En algunos casos, el modelo estad√≠stico que se desea aplicar requiere que los datos sigan una distribuci√≥n de probabilidad espec√≠fica, como la distribuci√≥n normal.

Por otro lado, muchas pruebas de hip√≥tesis utilizan **estad√≠sticos de prueba** (no necesariamente modelos completos). Por ejemplo:

-   Las **pruebas t** (t de Student) usan el estad√≠stico *t*.

-   El **ANOVA** usa el estad√≠stico *F*.

-   Las **pruebas de chi-cuadrado** usan el estad√≠stico œá¬≤.

Estas pruebas se basan en las distribuciones te√≥ricas de estos estad√≠sticos para calcular los valores p, los cuales permiten decidir si aceptar o rechazar la hip√≥tesis nula.

Este esta sesi√≥n pr√°ctica se enfocar√° en el segundo escenario.

# Cargamos los paquetes necesarios

```{r}
library(rio)
library(here)


```

# Cargar los datos

```{r}
data_cerv_0 <- import(here("data", "conoc_actit_factor_cancer_cervical.csv"))
```

# 1. Para datos continuos

La prueba t de Student y el ANOVA son dos pruebas estad√≠sticas ampliamente utilizadas que permiten evaluar si el valor promedio de una variable num√©rica difiere entre dos o m√°s grupos o categor√≠as.

Ambas pruebas asumen que la variable continua sigue una distribuci√≥n normal.\
Pero, ¬øc√≥mo podemos comprobar si esta condici√≥n se cumple?\
Mediante una prueba de bondad de ajuste.

Una de las pruebas m√°s comunes para evaluar la normalidad de una variable num√©rica es la prueba de Shapiro-Wilk. Esta prueba permite determinar si los datos provienen de una distribuci√≥n normal, lo cual es un requisito clave antes de aplicar pruebas como la t de Student o el ANOVA.

## Para la variable circun_cintura

Esta variable corresponde a medidas de circunferecia de cintura en centimetros. En R, usamos la funci√≥n nativa `shapiro.test()` para realizar la prueba de Shapiro-Wilk

```{r}
shapiro.test(data_glucosa_circun$circun_cintura)
```

## Para la variable glucosa

Esta variable corresponde a medidas de glucosa en mg/dL

```{r}
shapiro.test(data_glucosa_circun$glucosa)
```

## Respecto a la interpretaci√≥n de los dos resultados

Las hip√≥tesis de la prueba de Shapiro-Wilk

-   La hip√≥tesis nula (H‚ÇÄ) establece que la muestra proviene de una distribuci√≥n normal.

-   La hip√≥tesis alternativa (H‚ÇÅ) plantea que la muestra no proviene de una distribuci√≥n normal.

Si tomamos en cuenta que el valor de p aceptado para esta evaluaci√≥n es \< 0.05, entonces el resultado de la evaluaci√≥n de normalidad para la variable circunferecia de cintura indica que esta variable NO tiene una distribuci√≥n normal.

En contraste, el resultado para la variable glucosa (p = 0.7338) indica que la muestra s√≠ proviene de una distribuci√≥n normal.

# 2. Para datos categ√≥ricos

El dataset para esta sesi√≥n contiene informaci√≥n sobre el estado de s√≠ndrome metab√≥lico. En esta muestra, el n√∫mero de participantes con s√≠ndrome metab√≥lico es 65 de un total de 200.

```{r}
table(data_glucosa_circun$sindrom_metabolico)
```

Un estudio previo realizado en Per√∫ report√≥ una prevalencia de s√≠ndrome metab√≥lico del 26,9% (DOI: <https://doi.org/10.1111/j.1365-2362.2009.02191.x>).

En este caso, la prevalencia del estudio previo representa el valor esperado, mientras que la prevalencia observada en nuestro conjunto de datos representa el valor observado.

Uno de los objetivos de nuestro an√°lisis es evaluar si la proporci√≥n observada de s√≠ndrome metab√≥lico difiere significativamente de la proporci√≥n esperada. Para ello, utilizamos la prueba de bondad de ajuste de Chi-cuadrado.

Las hip√≥tesis de esta prueba son las siguientes:

-   **Hip√≥tesis nula (H‚ÇÄ):** No existe una diferencia significativa entre la proporci√≥n observada y la esperada.

-   **Hip√≥tesis alternativa (H‚ÇÅ):** Existe una diferencia significativa entre la proporci√≥n observada y la esperada.

En R, esta prueba se realiza mediante la funci√≥n `chisq.test()`, a la cual se deben proporcionar los valores observados y las proporciones esperadas para llevar a cabo la comparaci√≥n.

```{r}
chisq.test(x = c(65, 135), p = c(0.269, 0.731))
```

Interpretaci√≥n

Dado que el valor de p es mayor a 0.05, podemos concluir que las proporciones observadas no son significativamente diferentes de las proporciones esperadas.

# **9. Regresion Lineal Simple Prueba**

# Cargando los datos

```{r}
circun_glucosa <- import(here("data", "s09_circunf_glucosa.csv"))
```

# Sobre los datos para esta pr√°ctica

El dataset circun_glucosa, de 1000 personas adultas (\>=20 a√±os de edad), contiene datos glucosa medida en ayunas (en mg/dL), cirunferencia de cintura (en centimetros), tabaquismo y otros datos demogr√°ficos.

# 1 Regresi√≥n lineal simple

Regresi√≥n es una m√©todo para evaluar la asociaci√≥n entre una variable dependiente (tambien llamado desenlace Y) y una o varias variables independientes (predictoras X1, X2,..., Xk). Los modelos de regresi√≥n lineal simple (o univariable) utilizan solo solo una variable independiente o predictora X. Ejemplos de preguntas de investigaci√≥n se puede responder usando un modelo de regresi√≥n lineal:

-   ¬øExiste una asociaci√≥n entre el promedio final del curso de Metodos y Sistematizaci√≥n de M√©todos Estad√≠sticos (desenlace o variable dependiente) y las horas de sue√±o (preditor o variable independiente)?

-   ¬øExiste una asoaci√≥n entre el el nivel de glucosa y la circunferencia de cintura?

La ultima pregunta es la que evaluaremos en esta pr√°ctica.

## 1.1 El problema en este ejercicio

El desenlace *Y* de inter√©s para este ejercicio es la variable glucosa medida en ayunas. Veamos la distribuci√≥n de la variable y el promedio en en un histograma.

```{r}
circun_glucosa |>     ggplot(aes(x = glucosa_mg_dL)) +   geom_histogram(     color = "white",     ) +    labs(y = "Frecuencia",         x = "Glucosa (mg/dL)") +   geom_vline(xintercept = mean(circun_glucosa$glucosa_mg_dL, na.rm = TRUE),              color = "darkred", size = 1.5)
```

En estos datos, el promedio de la glucosa es:

```{r}
mean(circun_glucosa$glucosa_mg_dL, na.rm = TRUE)
```

Una observaci√≥n importante a partir del histograma y el promedio (el valor esperado) es que existe una gran variaci√≥n entre los valores de glucosa de los individuos de quienes provienen los datos. Podemos hipotetizar de que otras variables (predictores) podr√≠an influir en esta variaci√≥n, por ejemplo, la circunferencia de cintura.

## 1.2 Notaci√≥n en el m√©todo de regresi√≥n lineal simple

El m√©todo de regresi√≥n lineal simple encuentra la l√≠nea que mejor se ajusta a la descripci√≥n lineal entre la glucosa en ayunas y la circunferencia de la cintura, tal como se muestra en la siguiente figura:

```{r}
plot(glucosa_mg_dL ~ circunf_cintura_cm , data = circun_glucosa,      col = "gray",      ylab = "Glucosa (mg/dL)",      xlab = "Circunferencia de cintura (cm)",      las = 1,      pch = 20,       font.lab = 2, font.axis = 2)   # La funci√≥n lm() ajusta el modelo de regresi√≥n lineal abline(lm(glucosa_mg_dL ~ circunf_cintura_cm , data = circun_glucosa), lwd = 2, col = "darkred")
```

La ecuaci√≥n siguiente ecuaci√≥n describe un modelo de regresi√≥n lineal simple para ùëå usando un predictor continuo ùëã. $$ Y = \beta_0 + \beta_1 X + \epsilon $$ Cuando ajustamos un modelo de regresi√≥n lineal simple a nuestros datos, estimamos (hallamos) los par√°metros del modelo que mejor explican la relaci√≥n entre las dos variables (desenlace y predictor), incluyendo los coeficientes (Œ≤‚ÇÄ, Œ≤‚ÇÅ) y el error (ùúÄ), que representa la variabilidad no explicada por el modelo.

Para un predictor continuo, el intercepto (Œ≤‚ÇÄ) es el valor esperado de Y cuando X = 0 (es decir, el promedio del resultado cuando el predictor es cero). La pendiente (Œ≤‚ÇÅ) es el cambio promedio en Y por cada unidad de cambio en X. El t√©rmino de error (ùúÄ) representa la diferencia entre los valores observados y los valores predichos por el modelo.

Aplicado a nuestro ejemplo, el intercepto (Œ≤‚ÇÄ) representa la circunferencia de cintura promedio cuando la glucosa en ayunas es cero (aunque este valor puede no tener sentido pr√°ctico, es necesario matem√°ticamente). La pendiente (Œ≤‚ÇÅ) indica cu√°nto aumenta (o disminuye) en promedio la circunferencia de la cintura por cada unidad adicional de glucosa en ayunas (medida en mg/dL). El error (ùúÄ) recoge la variaci√≥n individual que no es explicada solo por la glucosa.

Asi que, como el objetivo es hallar los valores de los par√°metros (Œ≤‚ÇÄ,Œ≤‚ÇÅ,ùúÄ), es apropiado decir que estamos 'ajustando el modelo de regresi√≥n lineal simple' para el problema planteado (a.k.a la asociaci√≥n entre glucosa y la circunferencia de cintura)

## 1.3 Ajustando el modelo de regresi√≥n lineal simple para nuestro problema

En R, usamos la funci√≥n lm() para ajustar un modelo de regresi√≥n lineal. "lm" es la abreviatura para "linear model". Dentro de la funci√≥n debemos indicarle como argumentos el desenlace X, el predictor Y y la data donde se encuentran las variables. Esta es la estructura para ajustar el modelo con la funci√≥n lm: lm(y \~ x, data = mis_datos).

Ajustando el modelo para nuestros datos

```{r}
modelo_ejemplo = lm(glucosa_mg_dL ~ circunf_cintura_cm, data = circun_glucosa)
```

Para ver los resultados, usamos la funci√≥n summary() y dentro, el objeto modelo_ejemplo.

```{r}
summary(modelo_ejemplo)
```

## 1.4 Interpretando los resultados

La secci√≥n Coefficients del resultado:

```{r}
summary(modelo_ejemplo)$coef
```

...muestra las estimaciones y las pruebas de hip√≥tesis para el intercepto (Œ≤‚ÇÄ), etiquetado como (Intercept), y para el coeficiente de la circunferencia de cintura (la pendiente, Œ≤‚ÇÅ), etiquetado como Circunfe_brazo_cm.

En esta misma secci√≥n, la columna Estimate muestra los coeficientes estimados del modelo de regresi√≥n lineal simple. As√≠, el modelo que mejor se ajusta tiene un intercepto de 59.474 y una pendiente de 0.49970.

La tabla de coeficientes tambi√©n muestra el error est√°ndar de cada estimaci√≥n, su valor t y su valor p (etiquetado como Pr(\>\|t\|)). El valor p del intercepto usualmente no es de inter√©s, pero el valor p del predictor (Circunfe_brazo_cm) prueba la hip√≥tesis nula de que el desenlace NO tiene asociaci√≥n con el predictor o, dicho de otra manera, que la pendiente es cero. La hip√≥tesis nula plantea que la l√≠nea de mejor ajuste es una l√≠nea horizontal, lo que indicar√≠a que el promedio esperado del desenlace es el mismo en todos los valores del predictor; es decir, que no existe asociaci√≥n entre el desenlace (glucosa) y el predictor (circunferencia de cintura).

Finalmente, el valor R-cuadrado es una medida de bondad de ajuste que var√≠a entre 0 (sin asociaci√≥n) y 1 (asociaci√≥n lineal perfecta), y corresponde al cuadrado de la correlaci√≥n de Pearson entre el desenlace y el predictor. Se interpreta como la proporci√≥n de la variaci√≥n en el desenlace que es explicada por el modelo. En nuestro modelo, el R¬≤ (R-cuadrado) es 0.0871. Esto significa que aproximadamente el 8.6% de la variaci√≥n en los valores de glucosa en ayunas se explica por la circunferencia de la cintura

## 1.5 ¬øC√≥mo reportar los resultados del ajuste del modelo de regresi√≥n lineal simple?

Tanto si se trata de una tesis o un art√≠culo, abajo un ejemplo de c√≥mo reportar los resultados del presente problema:

> "(...) empleamos un modelo de regresi√≥n linear para evaluar la asociaci√≥n entre el nivel de glucosa en ayunas (mg/dL) y la circunferencia de cintura (cm) usando datos de 965 adultos. 8.71% de la variaci√≥n en el nivel de glucosa en ayunas fue explicada por la circunferencia de cintura (R¬≤=0.0871). Se encontr√≥ una asociaci√≥n positiva significativa entre la glucosa en ayunas y la circunferencia de cintura (B=0.499; p \<.001). En promedio, por cada diferencia de 1 cm en la circunferencia de cintura, los adultos difieren en el promedio de glucosa en ayunas en 0.499 mg/dL"

Adicionalmente, es buena idea presentar los resultados en un tabla.

```{r}
theme_gtsummary_language("es")

tabla_reporte <- modelo_ejemplo |> 
  tbl_regression(intercept = T,
                        estimate_fun = function(x) style_sigfig(x, digits = 4),
                        pvalue_fun   = function(x) style_pvalue(x, digits = 3),
                 label        = list(circunf_cintura_cm ~ "Circunferencia de cintura (cm)")) |>
  modify_caption("Regresi√≥n de la glucosa en ayunas (mg/dL) en funci√≥n de la circunferencia de cintura")

tabla_reporte
```

**Exportamos la tabla**

```{r}
tabla_reporte |>    as_flex_table()  |>    flextable::save_as_docx(path = "tabla_reporte.docx")
```

# 2 Prueba t de Student para muestras independientes

Imagina que, ahora, luego de haber tomado las mediciones de medidas de glucosa en ayunas (mg/dL) queremos saber si el promedio de glucosa en varones es significativamente diferente del promedio de glucosa en mujeres. Es esta situaci√≥n, hay dos grupos (varones y mujeres) de muestras independientes.

## 2.1 ¬øCu√°ndo usar la prueba t de Student para muestras independientes?

-   Cuando los dos grupos de muestras a comparar han sido muestreadas de una distribuci√≥n normal. Aqu√≠ podemos usar la prueba de Shapiro-Wilk.

-   Cuando las varianzas de los dos grupos son iguales. Esto puede ser evaluado con la prueba de Levene o la prueba F.

2.2 Usualmente, la hip√≥tesis de la prueba t de Student son:

-   Hip√≥tesis nula (H‚ÇÄ): No hay diferencia entre las medias de los dos grupos. $$ H_0: \mu_1 = \mu_2 $$
-   Hip√≥tesis alternativa (H‚ÇÅ): Hay una diferencia entre las medias de los dos grupos. $$ H_1: \mu_1 \neq \mu_2 $$

## 2.2 Sobre los datos para esta pr√°ctica

El dataset circun_glucosa, de 1000 personas adultas (\>=20 a√±os de edad), contiene datos circunferencia de cintura (en cent√≠metros), la variable sexo y otros datos demogr√°ficos.

## 2.3 Resumen y visualizaci√≥n

Resumen

Antes de realizar la prueba t de Student es importante conocer la distribuci√≥n de los datos e identificar si hay valores perdidos o at√≠picos. Empecemos por el resumen:

```{r}
group_by(circun_glucosa, sexo) |> 
  summarise(
    count = n(),
    mean = mean(circunf_brazo_cm, na.rm = TRUE),
    sd = sd(circunf_brazo_cm, na.rm = TRUE)
  )
```

Visualizaci√≥n

```{r}
circun_glucosa |>  
  filter(!is.na(sexo)& !is.na(circunf_brazo_cm))  |> 
  ggplot(aes(y = circunf_brazo_cm, x = sexo)) +
  geom_boxplot() +
  labs(y = "Circunferencia del brazo (cm)", x = "sexo")
```

## 2.4 Pruebas preliminares para evaluar los supuestos de la prueba t de Student

Supuesto 1: los datos deben haber sido muestreados de una distribuci√≥n normal.

Para esto, usamos la prueba de Shapiro-wilk.

```{r}
circun_glucosa |>    filter(sexo == "Masculino") |>    summarise(shapiro = list(shapiro.test(circunf_brazo_cm))) |>    pull(shapiro)
```

```{r}
circun_glucosa |>    filter(sexo == "Femenino") |>    summarise(shapiro = list(shapiro.test(circunf_brazo_cm))) |>    pull(shapiro)
```

Supuesto 2: Las varianzas de los dos grupos son iguales Para esto podemos usar la prueba F para evaluar la homogeneidad de varianzas. Esto esta implementado en la funci√≥n var.test()

```{r}
ls()

```

```         
```

```{r}
var.test(circunf_brazo_cm_sim ~ sexo, data = data_mod)
```

El valor p de la prueba F es p = 0.3143. Es mayor que el nivel de significancia Œ± = 0.05. En conclusi√≥n, no hay una diferencia significativa entre las varianzas de los dos conjuntos (femenino y masculino) de datos. Por lo tanto, podemos usar la prueba t cl√°sica que asume igualdad de varianzas.

## 2.5 Realizamos la prueba t para nuestros datos.

```{r}
t.test(circunf_brazo_cm ~ sexo, data = circun_glucosa, var.equal = TRUE)
```

**Interpretando los resultados**

El valor p de la prueba es 0.003615, lo cual es menor que el nivel de significancia Œ± = 0.05. Por lo tanto, podemos concluir que la circunferencia promedio del brazo en hombres es significativamente diferente de la circunferencia promedio en mujeres.

PC3-1\
SEMANA 10

# Cargamos e instalamos paquetes

```{r}
install.packages("car")
install.packages("cards")
install.packages("broom.helpers")
```

```{r}
library(tidyverse)
library(here)
library(rio)
library(gtsummary)
library(car)
library(cards)
library(broom.helpers)
```

## Cargando los datos

```{r}
hipert_covid <- import(here("data", "s10_hipert_covid.csv"))
```

```{r}
asma <- import(here("data", "s10_asma.csv"))
```

## 1.2 Estimando OR usando regresi√≥n log√≠stica para un predictor categ√≥rico

```{r}
hipert_covid_1 <- hipert_covid |> 
  mutate(hipert = relevel(as.factor(hipert), ref = "no"),
         desenlace = relevel(as.factor(desenlace), ref = "vivo"))
```

A continuaci√≥n, usamos la funci√≥n `glm()`, general linear model, con el argumento family = binomial para ajustar una regresi√≥n log√≠stica y `summary()` para ver los resultados.

```{r}
regre_log <- glm(desenlace ~ hipert,
                 family = binomial, 
                 data = hipert_covid_1)

summary(regre_log)
```

Para obtener el OR en s√≠ (como usualmente se reporta en los estudios), exponenciamos el coeficiente usando la funci√≥n exp()

```{r}
exp(coef(regre_log)[-1]) # [-1] elimina la primera fila, al intercepto.
```

Usamos la funci√≥n `confint()` para calcular los intervalos de confianza (IC) al 95% para el coeficientes de regresi√≥n, y exponenciamos estos valores para obtener los IC del 95% para los OR.

```{r}
exp(confint(regre_log))[-1, , drop=F]
```

## 1.4 Estimando OR usando regresi√≥n log√≠stica para un predictor num√©rico

```{r}
regre_log_1 <- glm(desenlace ~ edad, family = binomial, data = hipert_covid_1)

summary(regre_log_1)$coef
```

```{r}
exp(coef(regre_log_1)[-1])
```

```{r}
exp(confint(regre_log_1)[-1,])
```

```{r}
theme_gtsummary_language(language = "es")
```

```{r}
tabla_reg_logi <- hipert_covid_1 |>
  tbl_uvregression(
    include = c(edad, sexo, hipert),
    y = desenlace,
    method = glm,
    method.args = list(family = binomial),
    exponentiate = TRUE,
    conf.int = TRUE,
    hide_n = TRUE,
    add_estimate_to_reference_rows = FALSE,
    pvalue_fun = ~ style_pvalue(.x, digits = 3),
    estimate_fun = ~ style_number(.x, digits = 2),
    label = list(
      edad ~ "Edad (a√±os)",
      sexo ~ "Sexo",
      hipert ~ "Hipertensi√≥n"
    )
  ) |>
  bold_labels() |>
  bold_p(t = 0.05) |>
  modify_header(estimate = "**OR no ajustado**", p.value = "**Valor P**")
```

IMPRIMIMOS LAS TABLAS

```{r}
tabla_reg_logi
```

## 2.3 Ajustamos modelos de regresi√≥n de Poisson

```{r}
reg_poisson1 = glm(episod_asma ~ sexo, data = asma, family = "poisson")
summary(reg_poisson1)
```

```{r}
reg_poisson2 = glm(episod_asma ~ infec_resp_recur, data = asma, family = "poisson")
summary(reg_poisson2)
```

```{r}
reg_poisson2 = glm(episod_asma ~ ghq12, data = asma, family = "poisson")
summary(reg_poisson2)
```

## 2.4 C√≥mo interpretar y reportar los resultados de una regresi√≥n de Poisson

```{r}
tabla_reg_poisson <- asma |>
  tbl_uvregression(
    include = c(sexo, infec_resp_recur, ghq12),
    y = episod_asma,
    method = glm,
    method.args = list(family = poisson),
    exponentiate = TRUE,
    conf.int = TRUE,
    hide_n = TRUE,
    add_estimate_to_reference_rows = FALSE,
    pvalue_fun = ~ style_pvalue(.x, digits = 3),
    estimate_fun = ~ style_number(.x, digits = 2),
    label = list(
      sexo ~ "Sexo",
      infec_resp_recur ~ "Infecci√≥n respiratoria recurrente",
      ghq12 ~ "Bienestar psicol√≥gico"
    )
  ) |>
  bold_labels() |>
  bold_p(t = 0.05) |>
  modify_header(estimate = "**IRR no ajustado**", p.value = "**Valor P**")
```

```{r}
tabla_reg_poisson
```

Bas√°ndonos en esta tabla, podemos interpretar los resultados de la siguiente manera:

Ser del sexo femenino esta asociado a un menor riesgo de sufrir un ataque asm√°tico, con un IRR de 0.74 (IC 95%: 0.58, 0.94).

Aquellos con infecci√≥n respiratoria recurrente tienen un mayor riesgo de sufrir un ataque asm√°tico, con un IRR de 2.47 (IC 95%: 1.84, 3.26).

Un aumento de un punto en la puntuaci√≥n GHQ-12 (que evalua el bienestar psicol√≥gico) incrementa el riesgo de tener un ataque asm√°tico en 1.06 (IC 95%: 1.05, 1.06).
