---
title: "Proyecto del grupo 5"
author: "Lizeth Marcos"
format: html
editor: visual
Integrantes del grupo 5:
- Mantilla Saravia Daniel José
- Pachas Ventura Luis Marco
- Mendoza Felipa Astryd Xihomara
- Marcos Avalos Ruth Lizeth Edith
- Pachas Munayco Walter Manuel
- Acero Valencia Rodrigo
---

# Paquetes para visualizar datos

```{r}

install.packages("gridExtra")
install.packages("gtsummary")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("flextable")

```

```{r}
library(tidyverse)
library(rio)
library(here)
library(gridExtra) ## Para múltiple gráficos en una sola página
library(GGally) ## Para gráficos de correlación
library(forcats)
library(gtsummary)
library(dplyr)
library(ggplot2)
library(flextable)
```

# Cargando los datos

El *dataset* contiene datos de 218 pacientes con cáncer cervical. El *dataset* incluye 18 variables entre étnicas, ocupacionales y variables ocupacionales.

```{r}
data_cerv_0 <- import(here("data", "conoc_actit_factor_cancer_cervical.csv"))
```

## Examinamos los datos

`str()` es la función para ver la estructura de los datos.

```{r}
str(data_cerv_0)
```

```{r}
install.packages("skimr")
```

```{r}
library("skimr")
```

```{r}
skim(data_cerv)
```

## Conversión de caracteres a factor (categóricos) usando la función `mutate_if()`

Las variables categóricas (ej. Estadio T) han sido importadas como caracteres. Necesitamos transformalo a factores. En RStudio, factores es el tipo de dato para trabajar con variables categóricas.

```{r}
data_cerv <- data_cerv_0 |> 
  mutate_if(is.character, as.factor)
str(data_cerv)
```

# Visualizando datos: el molde

Para realizar visualizaciones con el paquete ggplot2, debemos reemplazar lo que esta encerrado en los signos. Este es el molde fundamental para crear gráficos más complejos.

`<midata> |> ggplot(aes(x = <var1>, y = <var2>)) + geom_<xxxx>()`

-   <midata> : el nombre del dataset a utilizar.
-   \|\> : esto es llamado "pipe", la cual conecta los datos a la función ggplot
-   \+ : usa + par conectar declaraciones de ggplot
-   <var> : la variable, cuyos datos serán usados para crear el gráfico.
-   geom\_<xxxx>: indica la función para crear el tipo de gráfico. Ej. geom_bar, para crear gráficos de barra.

# Visualizando distribución de datos

# 1. Visualizando datos categóricos

Gráficos de barra Los gráficos de barra son adecuados para mostrar frecuencias de variables categóricas.

```{r}
data_cerv |>  
  ggplot(aes(x = etnia)) +
  geom_bar()
```

Aquí, añadimos la función `fct_infreq()` de paquete forcats para ordenar (en orden decreciente) las barras del conteo, por estado marital.

```{r}
data_cerv |>  
  ggplot(aes(x = fct_infreq(etnia))) +
  geom_bar()
```

Con la función `labs()` podemos añadir nombres a los ejes del gráficos.

```{r}
data_cerv |>  
  ggplot(aes(x = fct_infreq(etnia))) +
  geom_bar() +
  labs(y = "Frecuencia", x = "etnia")
```

Para el gráfico de barra podemos usar frecuencias relativas. Por ejemplo, un gráfico de barras que muestre proporciones. Aquí es necesario calcular las proporciones. Nota que seguida a `y =` se muestra el cálculo para convertir los conteos a proporciones.

```{r}
data_cerv |>  
  ggplot(aes(x = etnia, y = ..count../sum(after_stat(count)))) +
  geom_bar() +
  labs(y = "Porcentaje", x = "etnia")
```

# 2. Visualizando Datos Numéricos

## 2.1. Con histogramas

Para visualizar conteos. Nota que aquí, la variable `edad` es numérica y la función para producir un histograma es `geom_histogram()`

```{r}
data_cerv |>  
  ggplot(aes(x = edad)) +
  geom_histogram() +
  labs(y = "Frecuencia", x = "edad")
```

Un histograma de proporciones. Aquí `..density..` es la estimación de densidad que reemplaza al conteo crudo. Toda el area del gráfico de densidad suma 1.

```{r}
data_cerv  |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(aes(y = ..density..)) +
  labs(y = "Density", x = "num_hijos")
```

A veces, puede ser util visualizar gráficos de lado a lado. Aquí dos histogramas lado a lado usando la función `grid.arrange()`

```{r}
hist_1 = data_cerv |> ggplot(aes(x = num_hijos)) +
  geom_histogram() +
  labs(y = "Frecuencia", x = "num_hijos")

hist_2 = data_cerv  |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(aes(y = ..density..)) +
  labs(y = "Density", x = "num_hijos")
```

```{r}
grid.arrange(hist_1, hist_2, ncol = 2)
```

Conteo con un número de barras distinto

Podemos cambiar los intervalos para la generación del histograma usando el argumento bins dentro de la función `geom_histogram()`

```{r}
data_cerv |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(bins = 30) +
  labs(y = "Frecuencia", x = "num_hijos")
```

Modificando los colores de las barras del histograma.

```{r}
data_cerv |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(
    color = "black", ## Color de las barras
    fill = "green" ## Color de las barras
    ) + 
  labs(y = "Frecuencia", 
       x = "num_hijos")
```

Modificando color en gráficos de barras. Nota que aquí, usamos el argumento fill para colorear las barras pertenecientes a las categorías.

```{r}
data_cerv |>  
  ggplot(aes(x = fct_infreq(etnia), fill = etnia)) +
  geom_bar() +
  labs(y = "Frecuencia", x = "etnia")
```

## 2.2. Con Boxplots (gráfico de cajas y bigotes)

Para mostrar datos de una variable en un gráfico de cajas y bigotes usamos la función `geom_boxplot()`

```{r}
data_mama |> 
  ggplot(aes(y = Albumina_g_dL)) + ## Cambia y por x para invertir el gráfico
  geom_boxplot() +
  theme(axis.text.x  = element_blank(),
        axis.ticks.x = element_blank()) +
  labs(y = "Albumina")
```

La función nativa de R, `boxplot()`, permite realizar el mismo gráfico.

```{r}
box_album_base = boxplot(data_mama$Albumina_g_dL,
                         ylab = "Albumina",
                         horizontal = TRUE, ## Cambia la dirección del gráfico
                         col = "salmon") ## Añadimos color
  
```

# 3. Visualizando variables categóricas *versus* categóricas

```{r}
data_cerv |> 
  ggplot(aes(x = etnia, fill = ocupacion)) +
  geom_bar(position = "dodge") + ## Sin este argumento, las barras estarán una sobre otras
  labs(y = "Frecuencia",  
       x = "etnia",
       fill = "ocupacion")
```

Qué esta mal con esto?

```{r}
data_mama |>   
  group_by(Estadio_T, Estado_des)  |>  
  count() |>   
  # Compute proportions within grade
  # n is the default variable created by count()
  group_by(Estadio_T) |>   
  mutate(Proportion = n/sum(n))  |> 
  ggplot(aes(x = Estadio_T, y = Proportion, fill = Estado_des)) +
  geom_bar(position = 'dodge', stat = 'identity') +
  labs(y = "Proportion",
       x = "Estadio T",
       fill = "Desenlace")
```

```{r}
addmargins(prop.table(
  table(data_mama$Estado_des, data_mama$Estadio_T), 
  margin = 2), 1)
```

# 4. Visualizando distribución de variables continuas *versus* categóricas

## 4.1. Gráficos de barras

```{r}
data_mama |> 
  filter(!is.na(Recep_estrogeno) & !is.na(Estadio_T)) |> 
  group_by(Recep_estrogeno, Estadio_T) |> 
  summarise(n = n(),
            promedio = mean(Ki67_express, na.rm = T),
            de = sd(Ki67_express, na.rm = T)) |> 
  ggplot(aes(x = Recep_estrogeno, y = promedio, fill = Estadio_T)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = promedio - de, ymax = promedio + de),
                width = 0.5, size = 0.75, position = position_dodge(0.9)) +
  labs(y = "Expresión de KI67", fill = "Estadio_T", x = "Recep_estrogeno")
```

## 4.1. Boxplots lado a lado

```{r}
data_mama |>  
  filter(!is.na(Tam_tumor)& !is.na(Estadio_T))  |> 
  ggplot(aes(y = Tam_tumor, x = Estadio_T)) +
  geom_boxplot() +
  labs(y = "Tamaño", x = "Estadio T")
```

```{r}
data_mama |>  
  filter(!is.na(Tam_tumor)& !is.na(Estadio_T) & !is.na(Recep_estrogeno))  |> 
  ggplot(aes(y = Tam_tumor, x = Estadio_T, fill = Recep_estrogeno)) +
  geom_boxplot() +
  labs(y = "Tamaño", x = "Estadio T")
```

```{r}
data_mama |>  
  filter(!is.na(Tam_tumor)& !is.na(Estadio_T) & !is.na(Recep_estrogeno))  |> 
  ggplot(aes(y = Tam_tumor, x = Recep_estrogeno, fill = Estadio_T)) +
  geom_boxplot() +
  labs(y = "Tamaño", x = "Receptor")
```

## 4.3. Filas de histogramas

```{r}
data_mama  |>  
  filter(!is.na(Estadio_T) & !is.na(Estado_des) & !is.na(hemoglobina_g_dL)) |>
  group_by(Estadio_N) |>  
  ggplot(aes(x = hemoglobina_g_dL)) +
  geom_histogram(aes(y = ..density..), bins = 20,
                 color = "black", fill = "white") +
  labs(x = "Hemoglobina (mg/dL)", y = "Proporción") +
  facet_wrap(~Estadio_N, nrow = 4) +
  ggtitle("Hemoglobina por Estadio N")
```

# 5. Visualización para variables continuas versus continuas

Usamos la función geom_point para generar gráficos de dispersión y visualizar la relación de dos varaibles numéricas

```{r}
data_mama |> 
  ggplot(aes(x = Albumina_g_dL, y = hemoglobina_g_dL)) +
  geom_point() +
  labs(x = "Albumina (mg/dL)", y = "Hemoglobina (mg/dL)")
```

La función geom_smoth añade una línea de regresión al gráfico. "lm" es para linear model

```{r}
data_mama |> 
  ggplot(aes(x = Albumina_g_dL, y = hemoglobina_g_dL)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Albumina (mg/dL)", y = "Hemoglobina (mg/dL)")
```

Aquí, función geom_smooth() añade una línea de tendencia suavizada al gráfico.

```{r}
data_mama |> 
  ggplot(aes(x = Albumina_g_dL, y = hemoglobina_g_dL)) +
  geom_point() +
  geom_smooth(stat = "smooth", se = TRUE) +
  labs(x = "Albumina (mg/dL)", y = "Hemoglobina (mg/dL)")
```

Finalmente, la función `ggpairs()` permite visualizar multiple variables numerica a la vez. Aquí, combinamos la funciónm select() (para seleccionar las variables numéricas) y ggpairs, para generar el gráfico y los coeficientes de correlación.

```{r}
data_mama |>  
  select(hemoglobina_g_dL, Albumina_g_dL, Supervivencia_meses) %>% 
  ggpairs(columnLabels = c("Hemoglobina", "Albumin", "Supervivencia meses"))
```

# 6. Exportando figuras

```{r}
ggsave(
  "nombre_de_objeto.png",
  width = 7, # Ancho
  height = 4, # Alto
  dpi = 300 # resolución
) 
```

# 7. Prueba de Ajuste

**1. Modelo estadístico**\
Es una representación matemática que describe relaciones entre variables. Se utiliza para hacer inferencias, predicciones o entender el comportamiento de los datos.

**2. Variable continua**\
Tipo de variable numérica que puede tomar un número infinito de valores dentro de un rango (por ejemplo, niveles de glucosa o circunferencia de cintura).

**3. Distribución de probabilidad**\
Describe cómo se distribuyen los valores posibles de una variable aleatoria. Algunas distribuciones comunes incluyen la normal, binomial y chi-cuadrado.

**4. Hipótesis nula (H₀)**\
Proposición que se plantea al inicio de una prueba estadística y que indica la ausencia de efecto o diferencia. Se rechaza o no según el valor p obtenido.

**5. Valor p**\
Probabilidad de obtener un resultado igual o más extremo que el observado, si la hipótesis nula fuera cierta. Un valor p menor a 0.05 suele considerarse estadísticamente significativo.

# ¿Que son las pruebas de bondad de ajuste?

Las pruebas de bondad de ajust**e** evalúan qué tan bien los datos observados se ajustan a los valores esperados según un modelo estadístico.

La bondad de ajuste puede evaluarse en al menos dos escenarios principales:

### 1. En modelos de regresión

Por ejemplo, un estudiante podría aplicar un modelo de regresión lineal para evaluar la relación entre el peso de los pacientes de un hospital y su nivel de glucosa. Para determinar si el modelo es adecuado para explicar esta relación, se puede calcular el estadístico de bondad de ajuste R².

El estadístico R² mide el porcentaje de variabilidad de la variable dependiente (en este caso, el nivel de glucosa) que es explicado por el modelo de regresión. Cuanto mayor sea el valor de R², mejor será el ajuste del modelo a los datos observados.

### 2. En distribuciones de probabilidad

En algunos casos, el modelo estadístico que se desea aplicar requiere que los datos sigan una distribución de probabilidad específica, como la distribución normal.

Por otro lado, muchas pruebas de hipótesis utilizan **estadísticos de prueba** (no necesariamente modelos completos). Por ejemplo:

-   Las **pruebas t** (t de Student) usan el estadístico *t*.

-   El **ANOVA** usa el estadístico *F*.

-   Las **pruebas de chi-cuadrado** usan el estadístico χ².

Estas pruebas se basan en las distribuciones teóricas de estos estadísticos para calcular los valores p, los cuales permiten decidir si aceptar o rechazar la hipótesis nula.

Este esta sesión práctica se enfocará en el segundo escenario.

# Cargamos los paquetes necesarios

```{r}
library(rio)
library(here)


```

# Cargar los datos

```{r}
data_cerv_0 <- import(here("data", "conoc_actit_factor_cancer_cervical.csv"))
```

# 1. Para datos continuos

La prueba t de Student y el ANOVA son dos pruebas estadísticas ampliamente utilizadas que permiten evaluar si el valor promedio de una variable numérica difiere entre dos o más grupos o categorías.

Ambas pruebas asumen que la variable continua sigue una distribución normal.\
Pero, ¿cómo podemos comprobar si esta condición se cumple?\
Mediante una prueba de bondad de ajuste.

Una de las pruebas más comunes para evaluar la normalidad de una variable numérica es la prueba de Shapiro-Wilk. Esta prueba permite determinar si los datos provienen de una distribución normal, lo cual es un requisito clave antes de aplicar pruebas como la t de Student o el ANOVA.

## Para la variable circun_cintura

Esta variable corresponde a medidas de circunferecia de cintura en centimetros. En R, usamos la función nativa `shapiro.test()` para realizar la prueba de Shapiro-Wilk

```{r}
shapiro.test(data_glucosa_circun$circun_cintura)
```

## Para la variable glucosa

Esta variable corresponde a medidas de glucosa en mg/dL

```{r}
shapiro.test(data_glucosa_circun$glucosa)
```

## Respecto a la interpretación de los dos resultados

Las hipótesis de la prueba de Shapiro-Wilk

-   La hipótesis nula (H₀) establece que la muestra proviene de una distribución normal.

-   La hipótesis alternativa (H₁) plantea que la muestra no proviene de una distribución normal.

Si tomamos en cuenta que el valor de p aceptado para esta evaluación es \< 0.05, entonces el resultado de la evaluación de normalidad para la variable circunferecia de cintura indica que esta variable NO tiene una distribución normal.

En contraste, el resultado para la variable glucosa (p = 0.7338) indica que la muestra sí proviene de una distribución normal.

# 2. Para datos categóricos

El dataset para esta sesión contiene información sobre el estado de síndrome metabólico. En esta muestra, el número de participantes con síndrome metabólico es 65 de un total de 200.

```{r}
table(data_glucosa_circun$sindrom_metabolico)
```

Un estudio previo realizado en Perú reportó una prevalencia de síndrome metabólico del 26,9% (DOI: <https://doi.org/10.1111/j.1365-2362.2009.02191.x>).

En este caso, la prevalencia del estudio previo representa el valor esperado, mientras que la prevalencia observada en nuestro conjunto de datos representa el valor observado.

Uno de los objetivos de nuestro análisis es evaluar si la proporción observada de síndrome metabólico difiere significativamente de la proporción esperada. Para ello, utilizamos la prueba de bondad de ajuste de Chi-cuadrado.

Las hipótesis de esta prueba son las siguientes:

-   **Hipótesis nula (H₀):** No existe una diferencia significativa entre la proporción observada y la esperada.

-   **Hipótesis alternativa (H₁):** Existe una diferencia significativa entre la proporción observada y la esperada.

En R, esta prueba se realiza mediante la función `chisq.test()`, a la cual se deben proporcionar los valores observados y las proporciones esperadas para llevar a cabo la comparación.

```{r}
chisq.test(x = c(65, 135), p = c(0.269, 0.731))
```

Interpretación

Dado que el valor de p es mayor a 0.05, podemos concluir que las proporciones observadas no son significativamente diferentes de las proporciones esperadas.

# **9. Regresion Lineal Simple Prueba**

# Cargando los datos

```{r}
circun_glucosa <- import(here("data", "s09_circunf_glucosa.csv"))
```

# Sobre los datos para esta práctica

El dataset circun_glucosa, de 1000 personas adultas (\>=20 años de edad), contiene datos glucosa medida en ayunas (en mg/dL), cirunferencia de cintura (en centimetros), tabaquismo y otros datos demográficos.

# 1 Regresión lineal simple

Regresión es una método para evaluar la asociación entre una variable dependiente (tambien llamado desenlace Y) y una o varias variables independientes (predictoras X1, X2,..., Xk). Los modelos de regresión lineal simple (o univariable) utilizan solo solo una variable independiente o predictora X. Ejemplos de preguntas de investigación se puede responder usando un modelo de regresión lineal:

-   ¿Existe una asociación entre el promedio final del curso de Metodos y Sistematización de Métodos Estadísticos (desenlace o variable dependiente) y las horas de sueño (preditor o variable independiente)?

-   ¿Existe una asoación entre el el nivel de glucosa y la circunferencia de cintura?

La ultima pregunta es la que evaluaremos en esta práctica.

## 1.1 El problema en este ejercicio

El desenlace *Y* de interés para este ejercicio es la variable glucosa medida en ayunas. Veamos la distribución de la variable y el promedio en en un histograma.

```{r}
circun_glucosa |>     ggplot(aes(x = glucosa_mg_dL)) +   geom_histogram(     color = "white",     ) +    labs(y = "Frecuencia",         x = "Glucosa (mg/dL)") +   geom_vline(xintercept = mean(circun_glucosa$glucosa_mg_dL, na.rm = TRUE),              color = "darkred", size = 1.5)
```

En estos datos, el promedio de la glucosa es:

```{r}
mean(circun_glucosa$glucosa_mg_dL, na.rm = TRUE)
```

Una observación importante a partir del histograma y el promedio (el valor esperado) es que existe una gran variación entre los valores de glucosa de los individuos de quienes provienen los datos. Podemos hipotetizar de que otras variables (predictores) podrían influir en esta variación, por ejemplo, la circunferencia de cintura.

## 1.2 Notación en el método de regresión lineal simple

El método de regresión lineal simple encuentra la línea que mejor se ajusta a la descripción lineal entre la glucosa en ayunas y la circunferencia de la cintura, tal como se muestra en la siguiente figura:

```{r}
plot(glucosa_mg_dL ~ circunf_cintura_cm , data = circun_glucosa,      col = "gray",      ylab = "Glucosa (mg/dL)",      xlab = "Circunferencia de cintura (cm)",      las = 1,      pch = 20,       font.lab = 2, font.axis = 2)   # La función lm() ajusta el modelo de regresión lineal abline(lm(glucosa_mg_dL ~ circunf_cintura_cm , data = circun_glucosa), lwd = 2, col = "darkred")
```

La ecuación siguiente ecuación describe un modelo de regresión lineal simple para 𝑌 usando un predictor continuo 𝑋. $$ Y = \beta_0 + \beta_1 X + \epsilon $$ Cuando ajustamos un modelo de regresión lineal simple a nuestros datos, estimamos (hallamos) los parámetros del modelo que mejor explican la relación entre las dos variables (desenlace y predictor), incluyendo los coeficientes (β₀, β₁) y el error (𝜀), que representa la variabilidad no explicada por el modelo.

Para un predictor continuo, el intercepto (β₀) es el valor esperado de Y cuando X = 0 (es decir, el promedio del resultado cuando el predictor es cero). La pendiente (β₁) es el cambio promedio en Y por cada unidad de cambio en X. El término de error (𝜀) representa la diferencia entre los valores observados y los valores predichos por el modelo.

Aplicado a nuestro ejemplo, el intercepto (β₀) representa la circunferencia de cintura promedio cuando la glucosa en ayunas es cero (aunque este valor puede no tener sentido práctico, es necesario matemáticamente). La pendiente (β₁) indica cuánto aumenta (o disminuye) en promedio la circunferencia de la cintura por cada unidad adicional de glucosa en ayunas (medida en mg/dL). El error (𝜀) recoge la variación individual que no es explicada solo por la glucosa.

Asi que, como el objetivo es hallar los valores de los parámetros (β₀,β₁,𝜀), es apropiado decir que estamos 'ajustando el modelo de regresión lineal simple' para el problema planteado (a.k.a la asociación entre glucosa y la circunferencia de cintura)

## 1.3 Ajustando el modelo de regresión lineal simple para nuestro problema

En R, usamos la función lm() para ajustar un modelo de regresión lineal. "lm" es la abreviatura para "linear model". Dentro de la función debemos indicarle como argumentos el desenlace X, el predictor Y y la data donde se encuentran las variables. Esta es la estructura para ajustar el modelo con la función lm: lm(y \~ x, data = mis_datos).

Ajustando el modelo para nuestros datos

```{r}
modelo_ejemplo = lm(glucosa_mg_dL ~ circunf_cintura_cm, data = circun_glucosa)
```

Para ver los resultados, usamos la función summary() y dentro, el objeto modelo_ejemplo.

```{r}
summary(modelo_ejemplo)
```

## 1.4 Interpretando los resultados

La sección Coefficients del resultado:

```{r}
summary(modelo_ejemplo)$coef
```

...muestra las estimaciones y las pruebas de hipótesis para el intercepto (β₀), etiquetado como (Intercept), y para el coeficiente de la circunferencia de cintura (la pendiente, β₁), etiquetado como Circunfe_brazo_cm.

En esta misma sección, la columna Estimate muestra los coeficientes estimados del modelo de regresión lineal simple. Así, el modelo que mejor se ajusta tiene un intercepto de 59.474 y una pendiente de 0.49970.

La tabla de coeficientes también muestra el error estándar de cada estimación, su valor t y su valor p (etiquetado como Pr(\>\|t\|)). El valor p del intercepto usualmente no es de interés, pero el valor p del predictor (Circunfe_brazo_cm) prueba la hipótesis nula de que el desenlace NO tiene asociación con el predictor o, dicho de otra manera, que la pendiente es cero. La hipótesis nula plantea que la línea de mejor ajuste es una línea horizontal, lo que indicaría que el promedio esperado del desenlace es el mismo en todos los valores del predictor; es decir, que no existe asociación entre el desenlace (glucosa) y el predictor (circunferencia de cintura).

Finalmente, el valor R-cuadrado es una medida de bondad de ajuste que varía entre 0 (sin asociación) y 1 (asociación lineal perfecta), y corresponde al cuadrado de la correlación de Pearson entre el desenlace y el predictor. Se interpreta como la proporción de la variación en el desenlace que es explicada por el modelo. En nuestro modelo, el R² (R-cuadrado) es 0.0871. Esto significa que aproximadamente el 8.6% de la variación en los valores de glucosa en ayunas se explica por la circunferencia de la cintura

## 1.5 ¿Cómo reportar los resultados del ajuste del modelo de regresión lineal simple?

Tanto si se trata de una tesis o un artículo, abajo un ejemplo de cómo reportar los resultados del presente problema:

> "(...) empleamos un modelo de regresión linear para evaluar la asociación entre el nivel de glucosa en ayunas (mg/dL) y la circunferencia de cintura (cm) usando datos de 965 adultos. 8.71% de la variación en el nivel de glucosa en ayunas fue explicada por la circunferencia de cintura (R²=0.0871). Se encontró una asociación positiva significativa entre la glucosa en ayunas y la circunferencia de cintura (B=0.499; p \<.001). En promedio, por cada diferencia de 1 cm en la circunferencia de cintura, los adultos difieren en el promedio de glucosa en ayunas en 0.499 mg/dL"

Adicionalmente, es buena idea presentar los resultados en un tabla.

```{r}
theme_gtsummary_language("es")

tabla_reporte <- modelo_ejemplo |> 
  tbl_regression(intercept = T,
                        estimate_fun = function(x) style_sigfig(x, digits = 4),
                        pvalue_fun   = function(x) style_pvalue(x, digits = 3),
                 label        = list(circunf_cintura_cm ~ "Circunferencia de cintura (cm)")) |>
  modify_caption("Regresión de la glucosa en ayunas (mg/dL) en función de la circunferencia de cintura")

tabla_reporte
```

**Exportamos la tabla**

```{r}
tabla_reporte |>    as_flex_table()  |>    flextable::save_as_docx(path = "tabla_reporte.docx")
```

# 2 Prueba t de Student para muestras independientes

Imagina que, ahora, luego de haber tomado las mediciones de medidas de glucosa en ayunas (mg/dL) queremos saber si el promedio de glucosa en varones es significativamente diferente del promedio de glucosa en mujeres. Es esta situación, hay dos grupos (varones y mujeres) de muestras independientes.

## 2.1 ¿Cuándo usar la prueba t de Student para muestras independientes?

-   Cuando los dos grupos de muestras a comparar han sido muestreadas de una distribución normal. Aquí podemos usar la prueba de Shapiro-Wilk.

-   Cuando las varianzas de los dos grupos son iguales. Esto puede ser evaluado con la prueba de Levene o la prueba F.

2.2 Usualmente, la hipótesis de la prueba t de Student son:

-   Hipótesis nula (H₀): No hay diferencia entre las medias de los dos grupos. $$ H_0: \mu_1 = \mu_2 $$
-   Hipótesis alternativa (H₁): Hay una diferencia entre las medias de los dos grupos. $$ H_1: \mu_1 \neq \mu_2 $$

## 2.2 Sobre los datos para esta práctica

El dataset circun_glucosa, de 1000 personas adultas (\>=20 años de edad), contiene datos circunferencia de cintura (en centímetros), la variable sexo y otros datos demográficos.

## 2.3 Resumen y visualización

Resumen

Antes de realizar la prueba t de Student es importante conocer la distribución de los datos e identificar si hay valores perdidos o atípicos. Empecemos por el resumen:

```{r}
group_by(circun_glucosa, sexo) |> 
  summarise(
    count = n(),
    mean = mean(circunf_brazo_cm, na.rm = TRUE),
    sd = sd(circunf_brazo_cm, na.rm = TRUE)
  )
```

Visualización

```{r}
circun_glucosa |>  
  filter(!is.na(sexo)& !is.na(circunf_brazo_cm))  |> 
  ggplot(aes(y = circunf_brazo_cm, x = sexo)) +
  geom_boxplot() +
  labs(y = "Circunferencia del brazo (cm)", x = "sexo")
```

## 2.4 Pruebas preliminares para evaluar los supuestos de la prueba t de Student

Supuesto 1: los datos deben haber sido muestreados de una distribución normal.

Para esto, usamos la prueba de Shapiro-wilk.

```{r}
circun_glucosa |>    filter(sexo == "Masculino") |>    summarise(shapiro = list(shapiro.test(circunf_brazo_cm))) |>    pull(shapiro)
```

```{r}
circun_glucosa |>    filter(sexo == "Femenino") |>    summarise(shapiro = list(shapiro.test(circunf_brazo_cm))) |>    pull(shapiro)
```

Supuesto 2: Las varianzas de los dos grupos son iguales Para esto podemos usar la prueba F para evaluar la homogeneidad de varianzas. Esto esta implementado en la función var.test()

```{r}
ls()

```

```         
```

```{r}
var.test(circunf_brazo_cm_sim ~ sexo, data = data_mod)
```

El valor p de la prueba F es p = 0.3143. Es mayor que el nivel de significancia α = 0.05. En conclusión, no hay una diferencia significativa entre las varianzas de los dos conjuntos (femenino y masculino) de datos. Por lo tanto, podemos usar la prueba t clásica que asume igualdad de varianzas.

## 2.5 Realizamos la prueba t para nuestros datos.

```{r}
t.test(circunf_brazo_cm ~ sexo, data = circun_glucosa, var.equal = TRUE)
```

**Interpretando los resultados**

El valor p de la prueba es 0.003615, lo cual es menor que el nivel de significancia α = 0.05. Por lo tanto, podemos concluir que la circunferencia promedio del brazo en hombres es significativamente diferente de la circunferencia promedio en mujeres.

PC3-1\
SEMANA 10

# Cargamos e instalamos paquetes

```{r}
install.packages("car")
install.packages("cards")
install.packages("broom.helpers")
```

```{r}
library(tidyverse)
library(here)
library(rio)
library(gtsummary)
library(car)
library(cards)
library(broom.helpers)
```

## Cargando los datos

```{r}
hipert_covid <- import(here("data", "s10_hipert_covid.csv"))
```

```{r}
asma <- import(here("data", "s10_asma.csv"))
```

## 1.2 Estimando OR usando regresión logística para un predictor categórico

```{r}
hipert_covid_1 <- hipert_covid |> 
  mutate(hipert = relevel(as.factor(hipert), ref = "no"),
         desenlace = relevel(as.factor(desenlace), ref = "vivo"))
```

A continuación, usamos la función `glm()`, general linear model, con el argumento family = binomial para ajustar una regresión logística y `summary()` para ver los resultados.

```{r}
regre_log <- glm(desenlace ~ hipert,
                 family = binomial, 
                 data = hipert_covid_1)

summary(regre_log)
```

Para obtener el OR en sí (como usualmente se reporta en los estudios), exponenciamos el coeficiente usando la función exp()

```{r}
exp(coef(regre_log)[-1]) # [-1] elimina la primera fila, al intercepto.
```

Usamos la función `confint()` para calcular los intervalos de confianza (IC) al 95% para el coeficientes de regresión, y exponenciamos estos valores para obtener los IC del 95% para los OR.

```{r}
exp(confint(regre_log))[-1, , drop=F]
```

## 1.4 Estimando OR usando regresión logística para un predictor numérico

```{r}
regre_log_1 <- glm(desenlace ~ edad, family = binomial, data = hipert_covid_1)

summary(regre_log_1)$coef
```

```{r}
exp(coef(regre_log_1)[-1])
```

```{r}
exp(confint(regre_log_1)[-1,])
```

```{r}
theme_gtsummary_language(language = "es")
```

```{r}
tabla_reg_logi <- hipert_covid_1 |>
  tbl_uvregression(
    include = c(edad, sexo, hipert),
    y = desenlace,
    method = glm,
    method.args = list(family = binomial),
    exponentiate = TRUE,
    conf.int = TRUE,
    hide_n = TRUE,
    add_estimate_to_reference_rows = FALSE,
    pvalue_fun = ~ style_pvalue(.x, digits = 3),
    estimate_fun = ~ style_number(.x, digits = 2),
    label = list(
      edad ~ "Edad (años)",
      sexo ~ "Sexo",
      hipert ~ "Hipertensión"
    )
  ) |>
  bold_labels() |>
  bold_p(t = 0.05) |>
  modify_header(estimate = "**OR no ajustado**", p.value = "**Valor P**")
```

IMPRIMIMOS LAS TABLAS

```{r}
tabla_reg_logi
```

## 2.3 Ajustamos modelos de regresión de Poisson

```{r}
reg_poisson1 = glm(episod_asma ~ sexo, data = asma, family = "poisson")
summary(reg_poisson1)
```

```{r}
reg_poisson2 = glm(episod_asma ~ infec_resp_recur, data = asma, family = "poisson")
summary(reg_poisson2)
```

```{r}
reg_poisson2 = glm(episod_asma ~ ghq12, data = asma, family = "poisson")
summary(reg_poisson2)
```

## 2.4 Cómo interpretar y reportar los resultados de una regresión de Poisson

```{r}
tabla_reg_poisson <- asma |>
  tbl_uvregression(
    include = c(sexo, infec_resp_recur, ghq12),
    y = episod_asma,
    method = glm,
    method.args = list(family = poisson),
    exponentiate = TRUE,
    conf.int = TRUE,
    hide_n = TRUE,
    add_estimate_to_reference_rows = FALSE,
    pvalue_fun = ~ style_pvalue(.x, digits = 3),
    estimate_fun = ~ style_number(.x, digits = 2),
    label = list(
      sexo ~ "Sexo",
      infec_resp_recur ~ "Infección respiratoria recurrente",
      ghq12 ~ "Bienestar psicológico"
    )
  ) |>
  bold_labels() |>
  bold_p(t = 0.05) |>
  modify_header(estimate = "**IRR no ajustado**", p.value = "**Valor P**")
```

```{r}
tabla_reg_poisson
```

Basándonos en esta tabla, podemos interpretar los resultados de la siguiente manera:

Ser del sexo femenino esta asociado a un menor riesgo de sufrir un ataque asmático, con un IRR de 0.74 (IC 95%: 0.58, 0.94).

Aquellos con infección respiratoria recurrente tienen un mayor riesgo de sufrir un ataque asmático, con un IRR de 2.47 (IC 95%: 1.84, 3.26).

Un aumento de un punto en la puntuación GHQ-12 (que evalua el bienestar psicológico) incrementa el riesgo de tener un ataque asmático en 1.06 (IC 95%: 1.05, 1.06).

## Instalar y cargar los paquetes

```{r}
install.packages("factoextra")
install.packages("cluster")
```

```{r}
library(factoextra)
library(cluster)
library(here)
library(rio)
library(tidyverse)
```

# 1 ¿Cómo aplicaremos Machine Learning a esta sesión?

Para intentar responder preguntas de investigación a veces es necesario que se realicen muchas medidas en una misma muestra. Por ejemplo, además de recolectar variables usuales como la edad, sexo y comobilidades, podríamos recolectar tambien varios otros parámetros laboratoriales como creatinina sérica, glucosa, hemoglobina glicosilada, y varios otros adicionales. Y lo cierto es que es posible que existan patrones entre los valores de las variables. Es decir, es posible que haya una dependencia entre las variables predictoras. Por ejemplo, si un grupo de pacientes tienen insuficiencia renal aguda, algunos parámetros renales de laboratorio tendrán valores fuera del rango normal, mientras que otros parámetros, no. Un opción para aplicar técnicas convencionales es la excluir variables redundantes o variables que podamos encontrar como "no interesantes". No obstante, esto puede llevar a pérdida de información. Para estas situaciones se pueden usar técnicas de machine learning como las técnicas de agrupamiento (clustering), la cual permitan la inclusión de multiple variables y permite definir grupos de pacientes que comparten similitudes respecto a las variables incluídas.

## 1.1 Uso de las técnicas de agrupamiento para responden preguntas de investigación en salud

Las técnicas de agrupamiento son un tipo de técnica exploratoria que puede usarse con el objetivo de clasificar observaciones (por ejemplo pacientes que forman parte de una muestra) en grupos en base a su similaridad y desimilaridad de las variables. A partir de esto, obtendremos grupos cuyos individuos que pertenecen a un mismo grupo son similares pero diferentes a individuos que pertenecen a otros grupos.

Los grupos encontrados pueden ser usados para hacer predicciones o evaluar diferencias en parámetros de laboratorio. Por ejemplo, entre grupos encontrados de pacientes quienes iniciaron su tratamiento para el cáncer, podemos comparar su supervivencia, calidad de vida luego de dos años u otras medidas a partir de los clusters (grupos) encontrados.

# 2 Análisis de agrupamiento herarquico (Hierarchical Clustering)

## 2.1 Sobre el problema para esta sesión

El dataset de esta sesión contiene información de 160 pacientes que han iniciado tratamiento de hemodiálisis en un hospital público de Lima, Perú. El dataset incluye variables numéricas de laboratorio que evalúan distintos perfiles clínicos, como el renal, hepático, electrolítico, lipídico, entre otros. El objetivo de este ejercicio es aplicar el método de agrupamiento jerárquico para identificar grupos de pacientes que compartan características similares en cuanto a su estado de salud basal, lo que permitirá proponer posibles categorías de riesgo o patrones clínicos diferenciados.

## 2.2 El dataset para esta sesión

Para ilustrar el proceso de análisis usaremos el dataset llamado `hemo_data` el cual contiene 160 observaciones con las siguientes variables: edad (años), sexo (masculino/femenino), enfermedad renal de base (variable categórica), peso corporal (kilogramos), talla (centímetros), índice de masa corporal (IMC, kilogramos/metro cuadrado), presión arterial sistólica (milímetros de mercurio, mmHg), presión arterial diastólica (mmHg), volumen urinario (mililitros por día), hemoglobina (gramos por decilitro, g/dL), recuento de leucocitos (miles por microlitro, 10³/μL), recuento de plaquetas (miles por microlitro, 10³/μL), proteínas totales (g/dL), albúmina (g/dL), aspartato aminotransferasa (AST, unidades por litro, U/L), alanina aminotransferasa (ALT, U/L), gamma-glutamil transferasa (γ-GTP, U/L), fosfatasa alcalina (U/L), ácido úrico sérico (miligramos por decilitro, mg/dL), nitrógeno ureico en sangre (BUN, mg/dL), creatinina sérica (mg/dL), sodio sérico (miliequivalentes por litro, mEq/L), potasio sérico (mEq/L), cloruro sérico (mEq/L), calcio sérico (mg/dL), fósforo sérico (mg/dL), magnesio sérico (mg/dL), proteína C-reactiva (mg/dL), colesterol total (mg/dL), triglicéridos (mg/dL), colesterol de lipoproteínas de alta densidad (HDL, mg/dL), hierro sérico (microgramos por decilitro, μg/dL), ferritina sérica (nanogramos por mililitro, ng/mL), capacidad insaturada de fijación de hierro (UIBC, μg/dL), péptido natriurético tipo B (BNP, picogramos por mililitro, pg/mL), hormona paratiroidea intacta (iPTH, pg/mL), glucosa sérica (mg/dL), hemoglobina glicosilada (HbA1c, porcentaje), β2-microglobulina (miligramos por litro, mg/L), gravedad específica urinaria (sin unidad), pH urinario (sin unidad), sodio urinario (mEq/L), potasio urinario (mEq/L), cloruro urinario (mEq/L), calcio urinario (mg/dL), fósforo urinario (mg/dL), magnesio urinario (mg/dL), nitrógeno ureico urinario (mg/dL), creatinina urinaria (mg/dL), ácido úrico urinario (mg/dL), proteína urinaria (mg/dL), N-acetil-β-D-glucosaminidasa (NAG, U/L), α1-microglobulina urinaria (mg/L) y proteína de unión a ácidos grasos en hígado tipo L (L-FABP, ng/mL)

### 2.2.1 Importando los datos

```{r}
hemo_data <- import(here("data", "s13_hemodialisis.csv"))
```

## 2.3 Preparación de los datos

### 2.3.1 Solo datos numéricos

Para el análisis de agrupamiento jerárquico de esta sesión usaremos solo variables numéricas. Es posible emplear variables categóricas en esta técnica, pero esto no será cubierto aquí. El código abajo elimina las variables categóricas `Sexo` y `Enfermedad_renal`. `id` será el identificador para los participantes.

```{r}
hemo_data_1 = hemo_data |> 
  select(-Sexo, -Enfermedad_Renal) |> 
  column_to_rownames("id")
```

### 2.3.2 La importancia de estandarizar

Adicionalmente, es fundamental estandarizar las variables antes de realizar el análisis de agrupamiento jerárquico. Estandarizar significa transformar las variables a una escala común para hacerlas comparables entre sí. Esto es especialmente importante porque uno de los pasos clave en el método de agrupamiento consiste en calcular distancias entre los objetos (en este caso, los pacientes) a partir de las variables clínicas incluidas en el dataset. Sin embargo, dichas variables se encuentran originalmente medidas en diferentes escalas y unidades. Por ejemplo, el índice de masa corporal (IMC) se expresa en kilogramos por metro cuadrado (kg/m²), mientras que la creatinina sérica se mide en miligramos por decilitro (mg/dL). Si no se realiza una estandarización previa, las variables con valores numéricos más grandes o con unidades distintas podrían influir desproporcionadamente en el cálculo de distancias, generando agrupamientos sesgados o poco representativos de la verdadera estructura de los datos.

Para ilustrar este punto: si se agrupa a los pacientes considerando simultáneamente su IMC (kg/m²) y su nivel de creatinina sérica (mg/dL), cabe preguntarse: ¿una diferencia de 1 kg/m² en IMC es tan relevante como una diferencia de 1 mg/dL en creatinina? ¿Qué variable debería tener mayor peso en la formación de los grupos? Sin una estandarización previa, estas diferencias no serían comparables, y las variables con mayor rango numérico dominarían el cálculo de distancias, afectando los resultados de la clasificación. Por ello, es imprescindible aplicar una función de estandarización, como `scale()` en R, que transforma las variables para que tengan media cero y desviación estándar uno, permitiendo así que todas contribuyan equitativamente al análisis.

```{r}
hemo_data_escalado = scale(hemo_data_1)
```

Un vistazo a los datos antes del escalamiento:

```{r}
head(hemo_data_1)
```

y un vistazo después del escalamiento:

```{r}
head(hemo_data_escalado)
```

## 2.4 Cálculo de distancias

Dado que uno de los pasos es encontrar "cosas similares", necesitamos definir "similar" en términos de distancia. Esta distancia la calcularemos para cada par posible de objetos (participantes) en nuestro dataset. Por ejemplo, si tuvieramos a los pacientes A, B y C, las distancia se calcularían para A vs B; A vs C; y B vs C. En R, podemos utilizar la función `dist()` para calcular la distancia entre cada par de objetos en un conjunto de datos. El resultado de este cálculo se conoce como matriz de distancias o de disimilitud.

```{r}
dist_hemo_data <- dist(hemo_data_escalado, method = "euclidean")
```

## 2.4.1 (opcional) Visualizando las distancias euclidianas con un mapa de calor

Una forma de visualizar si existen patrones de agrupamiento es usando mapas de calor (heatmaps). En R usamos la función `fviz_dist()` del paquete factoextra para crear un mapa de calor.

```{r}
fviz_dist(dist_hemo_data)
```

El nivel del color en este gráfico, es proporcional al valor de disimilaridad en observaciones (pacientes). Ejemplo, un color rojo puro indica una distancia con valor de 0 entre las abservaciones. Nota que la línea diagonal corresponde al intercepto de las mismas observaciones. Las observaciones que pertenecen a un mismo cluster (grupo) caen en orden consecutivo. Una conclusión del gráfico de abajo es que hay grupos que comparten similaridades dado que observamos grupos de colores.

## 2.5 El método de agrupamiento: función de enlace (linkage)

El agrupamiento jerárquico es un método que empieza agrupando las observaciones más parecidas entre sí, por lo que es fácil de usar al comienzo. Sin embargo, no basta con calcular las distancias entre todos los pares de objetos. Una vez que se forma un nuevo grupo (clúster), hay que decidir cómo medir la distancia entre ese grupo y los demás puntos o grupos ya existentes. Hay varias formas de hacerlo, y cada una genera un tipo diferente de agrupamiento jerárquico. La función de enlace (linkage) toma la información de distancias devuelta por la función `dist()` y agrupa pares de objetos en clústeres basándose en su similitud. Luego, estos nuevos clústeres formados se enlazan entre sí para crear clústeres más grandes. Este proceso se repite hasta que todos los objetos del conjunto de datos quedan agrupados en un único árbol jerárquico. Hay varios métodos para realizar este agrupamiento, incluyendo *Enlace máximo o completo*, *Enlace mínimo o simple*, *Enlace de la media o promedio*, *Enlace de centroide*, *Método de varianza mínima de Ward*. No entraremos en detalle sobre cómo funciona estos métodos, pero para este contexto el método de varianza minima de Ward o el método máximo, son preferidos. En este ejemplo, usamos el método de varianza mínima de Ward.

```{r}
dist_link_hemo_data <- hclust(d = dist_hemo_data, method = "ward.D2")
```

## 2.7 Dendrogramas para la visualización de patrones

Los dendrogramas es una representación gráfica del árbol jerárquico generado por la función `hclust()`.

```{r}
fviz_dend(dist_link_hemo_data, cex = 0.7)
```

Un dendrograma es como un árbol genealógico para los clústeres (grupos). Esta muestra cómo los puntos de datos individuales o los grupos de datos se van uniendo entre sí. En la parte inferior, cada punto de datos se representa como un grupo independiente, y a medida que se asciende, los grupos similares se combinan. Cuanto más bajo es el punto de unión, mayor es la similitud entre los grupos.

## 2.8 ¿Cúantos grupos se formaron en el dendrograma?

Uno de los problemas con la agrupación jerárquica es que no nos dice cuántos grupos hay ni dónde cortar el dendrograma para formar grupos. Aquí entra en juego la decisión del investigador a partir de analizar el dendrograma. Para nuestro dendrograma, es claro que el dendrograma muestra tres grupos. En el código de abajo, el argumento k = 3 define el número de clusters.

```{r}
fviz_dend(dist_link_hemo_data, 
          k = 3,
          cex = 0.5,
          k_colors = c("#2E9FDF", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, 
          rect = TRUE)
```

# 3 Agrupamiento con el algoritmo K-Means

El método de agrupamiento (usando el algoritmo) K-means es la técnica de machine learning más utilizado para dividir un conjunto de datos en un número determinado de k grupos (es decir, k clústeres), donde k representa el número de grupos predefinido por el investigador. Esto contrasta con la técnica anterior, dado que aquí sí iniciamos con un grupo pre-definido cuya idoniedad (de los grupos) puede ser evaluado. En detalle, el esta técnica clasifica a los objetos (participantes) del dataset en múltiples grupos, de manera que los objetos dentro de un mismo clúster sean lo más similares posible entre sí (alta similitud intragrupo), mientras que los objetos de diferentes clústeres sean lo más diferentes posible entre ellos (baja similitud intergrupo). En el agrupamiento k-means, cada clúster se representa por su centro (centroide), que corresponde al promedio de los puntos asignados a dicho clúster.

Aquí como funciona el algoritmo de K-Means

1.  Indicar cuántos grupos (clústeres) se quieren formar. Por ejemplo, si se desea dividir a los pacientes en 3 grupos según sus características clínicas, entonces K=3.
2.  Elegir aleatoriamente K casos del conjunto de datos como centros iniciales. Por ejemplo, R selecciona al azar 3 pacientes cuyas características (edad, IMC, creatinina, etc.) servirán como punto de partida para definir los grupos.
3.  Asignar cada paciente al grupo cuyo centro esté más cerca, usando la distancia euclidiana. Es como medir con una regla cuál centroide (paciente promedio) está más próximo a cada paciente en función de todas sus variables.
4.  Calcular un nuevo centro para cada grupo. Es decir, calcular el promedio de todas las variables de los pacientes que quedaron en ese grupo. Por ejemplo, si en el grupo 1 quedaron 40 pacientes, el nuevo centroide será el promedio de la edad, IMC, creatinina, etc., de esos 40 pacientes. Este centroide es un conjunto de valores (uno por cada variable).
5.  Repetir los pasos 3 y 4 hasta que los pacientes dejen de cambiar de grupo o hasta alcanzar un número máximo de repeticiones (en R, por defecto son 10 repeticiones). Esto permitirá que los grupos finales sean estables.

## 3.1 El problema y dataset para este ejercicio

Usaremos el mismo dataset y el mismo problema que el que empleamos en el ejercicio anterior (para Agrupamiento Jerárquico).

## 3.2 Estimando el número óptimo de clusters

Como indiqué arriba, el método de agrupamiento k-means requiere que el usuario especifique el número de clústeres (grupos) a generar. Una pregunta fundamental es: ¿cómo elegir el número adecuado de clústeres esperados (k)?

Aquí muestro una solución sencilla y popular: realizar el agrupamiento k-means probando diferentes valores de k (número de clústeres). Luego, se grafica la suma de cuadrados dentro de los clústeres (WSS) en función del número de clústeres. En R, podemos usar la función fviz_nbclust() para estimar el número óptimo de clústeres.

# 3 Agrupamiento con el algoritmo K-Means

El método de agrupamiento (usando el algoritmo) K-means es la técnica de machine learning más utilizado para dividir un conjunto de datos en un número determinado de k grupos (es decir, k clústeres), donde k representa el número de grupos predefinido por el investigador. Esto contrasta con la técnica anterior, dado que aquí sí iniciamos con un grupo pre-definido cuya idoniedad (de los grupos) puede ser evaluado. En detalle, el esta técnica clasifica a los objetos (participantes) del dataset en múltiples grupos, de manera que los objetos dentro de un mismo clúster sean lo más similares posible entre sí (alta similitud intragrupo), mientras que los objetos de diferentes clústeres sean lo más diferentes posible entre ellos (baja similitud intergrupo). En el agrupamiento k-means, cada clúster se representa por su centro (centroide), que corresponde al promedio de los puntos asignados a dicho clúster.

Aquí como funciona el algoritmo de K-Means

1.  Indicar cuántos grupos (clústeres) se quieren formar. Por ejemplo, si se desea dividir a los pacientes en 3 grupos según sus características clínicas, entonces K=3.
2.  Elegir aleatoriamente K casos del conjunto de datos como centros iniciales. Por ejemplo, R selecciona al azar 3 pacientes cuyas características (edad, IMC, creatinina, etc.) servirán como punto de partida para definir los grupos.
3.  Asignar cada paciente al grupo cuyo centro esté más cerca, usando la distancia euclidiana. Es como medir con una regla cuál centroide (paciente promedio) está más próximo a cada paciente en función de todas sus variables.
4.  Calcular un nuevo centro para cada grupo. Es decir, calcular el promedio de todas las variables de los pacientes que quedaron en ese grupo. Por ejemplo, si en el grupo 1 quedaron 40 pacientes, el nuevo centroide será el promedio de la edad, IMC, creatinina, etc., de esos 40 pacientes. Este centroide es un conjunto de valores (uno por cada variable).
5.  Repetir los pasos 3 y 4 hasta que los pacientes dejen de cambiar de grupo o hasta alcanzar un número máximo de repeticiones (en R, por defecto son 10 repeticiones). Esto permitirá que los grupos finales sean estables.

## 3.1 El problema y dataset para este ejercicio

Usaremos el mismo dataset y el mismo problema que el que empleamos en el ejercicio anterior (para Agrupamiento Jerárquico).

## 3.2 Estimando el número óptimo de clusters

Como indiqué arriba, el método de agrupamiento k-means requiere que el usuario especifique el número de clústeres (grupos) a generar. Una pregunta fundamental es: ¿cómo elegir el número adecuado de clústeres esperados (k)?

Aquí muestro una solución sencilla y popular: realizar el agrupamiento k-means probando diferentes valores de k (número de clústeres). Luego, se grafica la suma de cuadrados dentro de los clústeres (WSS) en función del número de clústeres. En R, podemos usar la función fviz_nbclust() para estimar el número óptimo de clústeres.

Primero escalamos los datos:

```{r}
hemo_data_escalado = scale(hemo_data_1)
```

Ahora graficamos la suma de cuadrados dentro de los gráficos

```{r}
fviz_nbclust(hemo_data_escalado, kmeans, nstart = 25, method = "wss") + 
  geom_vline(xintercept = 3, linetype = 2)
```

El punto donde la curva forma una "rodilla" o quiebre suele indicar el número óptimo de clústeres. Para nuestro gráfico, es en el número de cluster 3.

## 3.3 Cálculo del agrupamiento k-means

Dado que el resultado final del agrupamiento k-means es sensible a las asignaciones aleatorias iniciales, se especifica el argumento `nstart = 25`. Esto significa que R intentará 25 asignaciones aleatorias diferentes y seleccionará la mejor solución, es decir, aquella con la menor variación dentro de los clústeres. El valor predeterminado de `nstart` en R es 1. Sin embargo, se recomienda ampliamente utilizar un valor alto, como 25 o 50, para obtener un resultado más estable y confiable. El valor empleado aquí, fue usado para determinar el número de clústeres óptimos.

```{r}
set.seed(123)
km_res <- kmeans(hemo_data_escalado, 3, nstart = 25)
```

```{r}
km_res
```

El resultado muestra dos cosas:

1.  **Las medias o centros de los clústeres** (*Cluster means*): una matriz cuyas filas corresponden al número de clúster (1 a 3) y cuyas columnas representan las variables.

2.  **Un vector de asignación de clúster** (*Clustering vector*): un vector de números enteros (de 1 a 3) que indica a qué clúster ha sido asignado cada punto (para nuestro dataset, cada paciente).

## 3.4 Visualización de los clústeres k-means

Al igual que el análisis anterior, los datos se pueden representar en un gráfico de dispersión, coloreando cada observación o paciente según el clúster al que pertenece. El problema es que los datos contienen más de dos variables, y surge la pregunta de qué variables elegir para representar en los ejes X e Y del gráfico. Una solución es reducir la cantidad de dimensiones aplicando un algoritmo de reducción de dimensiones, como el Análisis de Componentes Principales (PCA). El PCA transforma las 52 variables originales en dos nuevas variables (componentes principales) que pueden usarse para construir el gráfico.

La función `fviz_cluster()` del paquete factoextra se puede usar para visualizar los clústeres generados por k-means. Esta función toma como argumentos los resultados del k-means y los datos originales (hemo_data_escalado).

```{r}
fviz_cluster
  km_res
  data = hemo_data_escalado
  palette = c("#2E9FDF", "#E7B800", "#FC4E07")
  ellipse.type = "euclid"
  repel = TRUE
  ggtheme = theme_minimal()
```

### 3.4.1 ¿Cómo interpretar?

En el gráfico resultante, los participantes (las observaciones) se representan como puntos. La técnica ha "creado" dimensiones, de las cuales dos de las más importantes de estas son consideradas en el gráfico. El uso aquí del PCA es poder clasificar diferentes "cosas" distintas en grupos, por ejemplo pacientes que iniciaron hemodialisis y que tienen distintos niveles de parámetros laboratoriales, de una manera que genere el menor error posible (en términos de predecir correctamente el tipo de célula). Además de los tres grupos formados (bien formados), nuestro gráfico aquí y en el agrupamiento jerárquico no nos dice más. Es necesario realizar análisis adicionales para evaluar la utilidad de estos clústeres, como por ejemplo, evaluar si la supervivencia entre estos tres grupos varia. O evaluar como, en promedio, varían parámetros importantes de laboratorio (por ejemplo creatinina y urea para evaluar la función renal).

########### 

**Aviso sobre el dataset de esta sesión:** A diferencia de sesiones anteriores, el conjunto de datos empleado en esta sesión es completamente simulado y no corresponde a información real de pacientes ni a datos provenientes de algún repositorio en línea. Es importante tener en cuenta que, en conjuntos de datos reales, los grupos formados mediante el análisis de agrupamiento pueden no ser tan claramente distinguibles como en estos ejemplos.
