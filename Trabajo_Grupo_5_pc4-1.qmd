---
title: "Proyecto del grupo 5"
author: "Lizeth Marcos"
format: html
editor: visual
Integrantes del grupo 5:
- Mantilla Saravia Daniel Jos√©
- Pachas Ventura Luis Marco
- Mendoza Felipa Astryd Xihomara
- Marcos Avalos Ruth Lizeth Edith
- Pachas Munayco Walter Manuel
- Acero Valencia Rodrigo
---

# Paquetes para visualizar datos

```{r}

install.packages("gridExtra")
install.packages("gtsummary")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("flextable")

```

```{r}
library(tidyverse)
library(rio)
library(here)
library(gridExtra) ## Para m√∫ltiple gr√°ficos en una sola p√°gina
library(GGally) ## Para gr√°ficos de correlaci√≥n
library(forcats)
library(gtsummary)
library(dplyr)
library(ggplot2)
library(flextable)
```

# Cargando los datos

El *dataset* contiene datos de 218 pacientes con c√°ncer cervical. El *dataset* incluye 18 variables entre √©tnicas, ocupacionales y variables ocupacionales.

```{r}
data_cerv_0 <- import(here("data", "conoc_actit_factor_cancer_cervical.csv"))
```

## Examinamos los datos

`str()` es la funci√≥n para ver la estructura de los datos.

```{r}
str(data_cerv_0)
```

```{r}
install.packages("skimr")
```

```{r}
library("skimr")
```

```{r}
skim(data_cerv)
```

## Conversi√≥n de caracteres a factor (categ√≥ricos) usando la funci√≥n `mutate_if()`

Las variables categ√≥ricas (ej. Estadio T) han sido importadas como caracteres. Necesitamos transformalo a factores. En RStudio, factores es el tipo de dato para trabajar con variables categ√≥ricas.

```{r}
data_cerv <- data_cerv_0 |> 
  mutate_if(is.character, as.factor)
str(data_cerv)
```

# Visualizando datos: el molde

Para realizar visualizaciones con el paquete ggplot2, debemos reemplazar lo que esta encerrado en los signos. Este es el molde fundamental para crear gr√°ficos m√°s complejos.

`<midata> |> ggplot(aes(x = <var1>, y = <var2>)) + geom_<xxxx>()`

-   <midata> : el nombre del dataset a utilizar.
-   \|\> : esto es llamado "pipe", la cual conecta los datos a la funci√≥n ggplot
-   \+ : usa + par conectar declaraciones de ggplot
-   <var> : la variable, cuyos datos ser√°n usados para crear el gr√°fico.
-   geom\_<xxxx>: indica la funci√≥n para crear el tipo de gr√°fico. Ej. geom_bar, para crear gr√°ficos de barra.

# Visualizando distribuci√≥n de datos

# 1. Visualizando datos categ√≥ricos

Gr√°ficos de barra Los gr√°ficos de barra son adecuados para mostrar frecuencias de variables categ√≥ricas.

```{r}
data_cerv |>  
  ggplot(aes(x = etnia)) +
  geom_bar()
```

Aqu√≠, a√±adimos la funci√≥n `fct_infreq()` de paquete forcats para ordenar (en orden decreciente) las barras del conteo, por estado marital.

```{r}
data_cerv |>  
  ggplot(aes(x = fct_infreq(etnia))) +
  geom_bar()
```

Con la funci√≥n `labs()` podemos a√±adir nombres a los ejes del gr√°ficos.

```{r}
data_cerv |>  
  ggplot(aes(x = fct_infreq(etnia))) +
  geom_bar() +
  labs(y = "Frecuencia", x = "etnia")
```

Para el gr√°fico de barra podemos usar frecuencias relativas. Por ejemplo, un gr√°fico de barras que muestre proporciones. Aqu√≠ es necesario calcular las proporciones. Nota que seguida a `y =` se muestra el c√°lculo para convertir los conteos a proporciones.

```{r}
data_cerv |>  
  ggplot(aes(x = etnia, y = ..count../sum(after_stat(count)))) +
  geom_bar() +
  labs(y = "Porcentaje", x = "etnia")
```

# 2. Visualizando Datos Num√©ricos

## 2.1. Con histogramas

Para visualizar conteos. Nota que aqu√≠, la variable `edad` es num√©rica y la funci√≥n para producir un histograma es `geom_histogram()`

```{r}
data_cerv |>  
  ggplot(aes(x = edad)) +
  geom_histogram() +
  labs(y = "Frecuencia", x = "edad")
```

Un histograma de proporciones. Aqu√≠ `..density..` es la estimaci√≥n de densidad que reemplaza al conteo crudo. Toda el area del gr√°fico de densidad suma 1.

```{r}
data_cerv  |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(aes(y = ..density..)) +
  labs(y = "Density", x = "num_hijos")
```

A veces, puede ser util visualizar gr√°ficos de lado a lado. Aqu√≠ dos histogramas lado a lado usando la funci√≥n `grid.arrange()`

```{r}
hist_1 = data_cerv |> ggplot(aes(x = num_hijos)) +
  geom_histogram() +
  labs(y = "Frecuencia", x = "num_hijos")

hist_2 = data_cerv  |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(aes(y = ..density..)) +
  labs(y = "Density", x = "num_hijos")
```

```{r}
grid.arrange(hist_1, hist_2, ncol = 2)
```

Conteo con un n√∫mero de barras distinto

Podemos cambiar los intervalos para la generaci√≥n del histograma usando el argumento bins dentro de la funci√≥n `geom_histogram()`

```{r}
data_cerv |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(bins = 30) +
  labs(y = "Frecuencia", x = "num_hijos")
```

Modificando los colores de las barras del histograma.

```{r}
data_cerv |>  
  ggplot(aes(x = num_hijos)) +
  geom_histogram(
    color = "black", ## Color de las barras
    fill = "green" ## Color de las barras
    ) + 
  labs(y = "Frecuencia", 
       x = "num_hijos")
```

Modificando color en gr√°ficos de barras. Nota que aqu√≠, usamos el argumento fill para colorear las barras pertenecientes a las categor√≠as.

```{r}
data_cerv |>  
  ggplot(aes(x = fct_infreq(etnia), fill = etnia)) +
  geom_bar() +
  labs(y = "Frecuencia", x = "etnia")
```

## 2.2. Con Boxplots (gr√°fico de cajas y bigotes)

Para mostrar datos de una variable en un gr√°fico de cajas y bigotes usamos la funci√≥n `geom_boxplot()`

```{r}
data_mama |> 
  ggplot(aes(y = Albumina_g_dL)) + ## Cambia y por x para invertir el gr√°fico
  geom_boxplot() +
  theme(axis.text.x  = element_blank(),
        axis.ticks.x = element_blank()) +
  labs(y = "Albumina")
```

La funci√≥n nativa de R, `boxplot()`, permite realizar el mismo gr√°fico.

```{r}
box_album_base = boxplot(data_mama$Albumina_g_dL,
                         ylab = "Albumina",
                         horizontal = TRUE, ## Cambia la direcci√≥n del gr√°fico
                         col = "salmon") ## A√±adimos color
  
```

# 3. Visualizando variables categ√≥ricas *versus* categ√≥ricas

```{r}
data_cerv |> 
  ggplot(aes(x = etnia, fill = ocupacion)) +
  geom_bar(position = "dodge") + ## Sin este argumento, las barras estar√°n una sobre otras
  labs(y = "Frecuencia",  
       x = "etnia",
       fill = "ocupacion")
```

Qu√© esta mal con esto?

```{r}
data_mama |>   
  group_by(Estadio_T, Estado_des)  |>  
  count() |>   
  # Compute proportions within grade
  # n is the default variable created by count()
  group_by(Estadio_T) |>   
  mutate(Proportion = n/sum(n))  |> 
  ggplot(aes(x = Estadio_T, y = Proportion, fill = Estado_des)) +
  geom_bar(position = 'dodge', stat = 'identity') +
  labs(y = "Proportion",
       x = "Estadio T",
       fill = "Desenlace")
```

```{r}
addmargins(prop.table(
  table(data_mama$Estado_des, data_mama$Estadio_T), 
  margin = 2), 1)
```

# 4. Visualizando distribuci√≥n de variables continuas *versus* categ√≥ricas

## 4.1. Gr√°ficos de barras

```{r}
data_mama |> 
  filter(!is.na(Recep_estrogeno) & !is.na(Estadio_T)) |> 
  group_by(Recep_estrogeno, Estadio_T) |> 
  summarise(n = n(),
            promedio = mean(Ki67_express, na.rm = T),
            de = sd(Ki67_express, na.rm = T)) |> 
  ggplot(aes(x = Recep_estrogeno, y = promedio, fill = Estadio_T)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymin = promedio - de, ymax = promedio + de),
                width = 0.5, size = 0.75, position = position_dodge(0.9)) +
  labs(y = "Expresi√≥n de KI67", fill = "Estadio_T", x = "Recep_estrogeno")
```

## 4.1. Boxplots lado a lado

```{r}
data_mama |>  
  filter(!is.na(Tam_tumor)& !is.na(Estadio_T))  |> 
  ggplot(aes(y = Tam_tumor, x = Estadio_T)) +
  geom_boxplot() +
  labs(y = "Tama√±o", x = "Estadio T")
```

```{r}
data_mama |>  
  filter(!is.na(Tam_tumor)& !is.na(Estadio_T) & !is.na(Recep_estrogeno))  |> 
  ggplot(aes(y = Tam_tumor, x = Estadio_T, fill = Recep_estrogeno)) +
  geom_boxplot() +
  labs(y = "Tama√±o", x = "Estadio T")
```

```{r}
data_mama |>  
  filter(!is.na(Tam_tumor)& !is.na(Estadio_T) & !is.na(Recep_estrogeno))  |> 
  ggplot(aes(y = Tam_tumor, x = Recep_estrogeno, fill = Estadio_T)) +
  geom_boxplot() +
  labs(y = "Tama√±o", x = "Receptor")
```

## 4.3. Filas de histogramas

```{r}
data_mama  |>  
  filter(!is.na(Estadio_T) & !is.na(Estado_des) & !is.na(hemoglobina_g_dL)) |>
  group_by(Estadio_N) |>  
  ggplot(aes(x = hemoglobina_g_dL)) +
  geom_histogram(aes(y = ..density..), bins = 20,
                 color = "black", fill = "white") +
  labs(x = "Hemoglobina (mg/dL)", y = "Proporci√≥n") +
  facet_wrap(~Estadio_N, nrow = 4) +
  ggtitle("Hemoglobina por Estadio N")
```

# 5. Visualizaci√≥n para variables continuas versus continuas

Usamos la funci√≥n geom_point para generar gr√°ficos de dispersi√≥n y visualizar la relaci√≥n de dos varaibles num√©ricas

```{r}
data_mama |> 
  ggplot(aes(x = Albumina_g_dL, y = hemoglobina_g_dL)) +
  geom_point() +
  labs(x = "Albumina (mg/dL)", y = "Hemoglobina (mg/dL)")
```

La funci√≥n geom_smoth a√±ade una l√≠nea de regresi√≥n al gr√°fico. "lm" es para linear model

```{r}
data_mama |> 
  ggplot(aes(x = Albumina_g_dL, y = hemoglobina_g_dL)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Albumina (mg/dL)", y = "Hemoglobina (mg/dL)")
```

Aqu√≠, funci√≥n geom_smooth() a√±ade una l√≠nea de tendencia suavizada al gr√°fico.

```{r}
data_mama |> 
  ggplot(aes(x = Albumina_g_dL, y = hemoglobina_g_dL)) +
  geom_point() +
  geom_smooth(stat = "smooth", se = TRUE) +
  labs(x = "Albumina (mg/dL)", y = "Hemoglobina (mg/dL)")
```

Finalmente, la funci√≥n `ggpairs()` permite visualizar multiple variables numerica a la vez. Aqu√≠, combinamos la funci√≥nm select() (para seleccionar las variables num√©ricas) y ggpairs, para generar el gr√°fico y los coeficientes de correlaci√≥n.

```{r}
data_mama |>  
  select(hemoglobina_g_dL, Albumina_g_dL, Supervivencia_meses) %>% 
  ggpairs(columnLabels = c("Hemoglobina", "Albumin", "Supervivencia meses"))
```

# 6. Exportando figuras

```{r}
ggsave(
  "nombre_de_objeto.png",
  width = 7, # Ancho
  height = 4, # Alto
  dpi = 300 # resoluci√≥n
) 
```

# 7. Prueba de Ajuste

**1. Modelo estad√≠stico**\
Es una representaci√≥n matem√°tica que describe relaciones entre variables. Se utiliza para hacer inferencias, predicciones o entender el comportamiento de los datos.

**2. Variable continua**\
Tipo de variable num√©rica que puede tomar un n√∫mero infinito de valores dentro de un rango (por ejemplo, niveles de glucosa o circunferencia de cintura).

**3. Distribuci√≥n de probabilidad**\
Describe c√≥mo se distribuyen los valores posibles de una variable aleatoria. Algunas distribuciones comunes incluyen la normal, binomial y chi-cuadrado.

**4. Hip√≥tesis nula (H‚ÇÄ)**\
Proposici√≥n que se plantea al inicio de una prueba estad√≠stica y que indica la ausencia de efecto o diferencia. Se rechaza o no seg√∫n el valor p obtenido.

**5. Valor p**\
Probabilidad de obtener un resultado igual o m√°s extremo que el observado, si la hip√≥tesis nula fuera cierta. Un valor p menor a 0.05 suele considerarse estad√≠sticamente significativo.

# ¬øQue son las pruebas de bondad de ajuste?

Las pruebas de bondad de ajust**e** eval√∫an qu√© tan bien los datos observados se ajustan a los valores esperados seg√∫n un modelo estad√≠stico.

La bondad de ajuste puede evaluarse en al menos dos escenarios principales:

### 1. En modelos de regresi√≥n

Por ejemplo, un estudiante podr√≠a aplicar un modelo de regresi√≥n lineal para evaluar la relaci√≥n entre el peso de los pacientes de un hospital y su nivel de glucosa. Para determinar si el modelo es adecuado para explicar esta relaci√≥n, se puede calcular el estad√≠stico de bondad de ajuste R¬≤.

El estad√≠stico R¬≤ mide el porcentaje de variabilidad de la variable dependiente (en este caso, el nivel de glucosa) que es explicado por el modelo de regresi√≥n. Cuanto mayor sea el valor de R¬≤, mejor ser√° el ajuste del modelo a los datos observados.

### 2. En distribuciones de probabilidad

En algunos casos, el modelo estad√≠stico que se desea aplicar requiere que los datos sigan una distribuci√≥n de probabilidad espec√≠fica, como la distribuci√≥n normal.

Por otro lado, muchas pruebas de hip√≥tesis utilizan **estad√≠sticos de prueba** (no necesariamente modelos completos). Por ejemplo:

-   Las **pruebas t** (t de Student) usan el estad√≠stico *t*.

-   El **ANOVA** usa el estad√≠stico *F*.

-   Las **pruebas de chi-cuadrado** usan el estad√≠stico œá¬≤.

Estas pruebas se basan en las distribuciones te√≥ricas de estos estad√≠sticos para calcular los valores p, los cuales permiten decidir si aceptar o rechazar la hip√≥tesis nula.

Este esta sesi√≥n pr√°ctica se enfocar√° en el segundo escenario.

# Cargamos los paquetes necesarios

```{r}
library(rio)
library(here)


```

# Cargar los datos

```{r}
data_cerv_0 <- import(here("data", "conoc_actit_factor_cancer_cervical.csv"))
```

# 1. Para datos continuos

La prueba t de Student y el ANOVA son dos pruebas estad√≠sticas ampliamente utilizadas que permiten evaluar si el valor promedio de una variable num√©rica difiere entre dos o m√°s grupos o categor√≠as.

Ambas pruebas asumen que la variable continua sigue una distribuci√≥n normal.\
Pero, ¬øc√≥mo podemos comprobar si esta condici√≥n se cumple?\
Mediante una prueba de bondad de ajuste.

Una de las pruebas m√°s comunes para evaluar la normalidad de una variable num√©rica es la prueba de Shapiro-Wilk. Esta prueba permite determinar si los datos provienen de una distribuci√≥n normal, lo cual es un requisito clave antes de aplicar pruebas como la t de Student o el ANOVA.

## Para la variable circun_cintura

Esta variable corresponde a medidas de circunferecia de cintura en centimetros. En R, usamos la funci√≥n nativa `shapiro.test()` para realizar la prueba de Shapiro-Wilk

```{r}
shapiro.test(data_glucosa_circun$circun_cintura)
```

## Para la variable glucosa

Esta variable corresponde a medidas de glucosa en mg/dL

```{r}
shapiro.test(data_glucosa_circun$glucosa)
```

## Respecto a la interpretaci√≥n de los dos resultados

Las hip√≥tesis de la prueba de Shapiro-Wilk

-   La hip√≥tesis nula (H‚ÇÄ) establece que la muestra proviene de una distribuci√≥n normal.

-   La hip√≥tesis alternativa (H‚ÇÅ) plantea que la muestra no proviene de una distribuci√≥n normal.

Si tomamos en cuenta que el valor de p aceptado para esta evaluaci√≥n es \< 0.05, entonces el resultado de la evaluaci√≥n de normalidad para la variable circunferecia de cintura indica que esta variable NO tiene una distribuci√≥n normal.

En contraste, el resultado para la variable glucosa (p = 0.7338) indica que la muestra s√≠ proviene de una distribuci√≥n normal.

# 2. Para datos categ√≥ricos

El dataset para esta sesi√≥n contiene informaci√≥n sobre el estado de s√≠ndrome metab√≥lico. En esta muestra, el n√∫mero de participantes con s√≠ndrome metab√≥lico es 65 de un total de 200.

```{r}
table(data_glucosa_circun$sindrom_metabolico)
```

Un estudio previo realizado en Per√∫ report√≥ una prevalencia de s√≠ndrome metab√≥lico del 26,9% (DOI: <https://doi.org/10.1111/j.1365-2362.2009.02191.x>).

En este caso, la prevalencia del estudio previo representa el valor esperado, mientras que la prevalencia observada en nuestro conjunto de datos representa el valor observado.

Uno de los objetivos de nuestro an√°lisis es evaluar si la proporci√≥n observada de s√≠ndrome metab√≥lico difiere significativamente de la proporci√≥n esperada. Para ello, utilizamos la prueba de bondad de ajuste de Chi-cuadrado.

Las hip√≥tesis de esta prueba son las siguientes:

-   **Hip√≥tesis nula (H‚ÇÄ):** No existe una diferencia significativa entre la proporci√≥n observada y la esperada.

-   **Hip√≥tesis alternativa (H‚ÇÅ):** Existe una diferencia significativa entre la proporci√≥n observada y la esperada.

En R, esta prueba se realiza mediante la funci√≥n `chisq.test()`, a la cual se deben proporcionar los valores observados y las proporciones esperadas para llevar a cabo la comparaci√≥n.

```{r}
chisq.test(x = c(65, 135), p = c(0.269, 0.731))
```

Interpretaci√≥n

Dado que el valor de p es mayor a 0.05, podemos concluir que las proporciones observadas no son significativamente diferentes de las proporciones esperadas.

# **9. Regresion Lineal Simple Prueba**

# Cargando los datos

```{r}
circun_glucosa <- import(here("data", "s09_circunf_glucosa.csv"))
```

# Sobre los datos para esta pr√°ctica

El dataset circun_glucosa, de 1000 personas adultas (\>=20 a√±os de edad), contiene datos glucosa medida en ayunas (en mg/dL), cirunferencia de cintura (en centimetros), tabaquismo y otros datos demogr√°ficos.

# 1 Regresi√≥n lineal simple

Regresi√≥n es una m√©todo para evaluar la asociaci√≥n entre una variable dependiente (tambien llamado desenlace Y) y una o varias variables independientes (predictoras X1, X2,..., Xk). Los modelos de regresi√≥n lineal simple (o univariable) utilizan solo solo una variable independiente o predictora X. Ejemplos de preguntas de investigaci√≥n se puede responder usando un modelo de regresi√≥n lineal:

-   ¬øExiste una asociaci√≥n entre el promedio final del curso de Metodos y Sistematizaci√≥n de M√©todos Estad√≠sticos (desenlace o variable dependiente) y las horas de sue√±o (preditor o variable independiente)?

-   ¬øExiste una asoaci√≥n entre el el nivel de glucosa y la circunferencia de cintura?

La ultima pregunta es la que evaluaremos en esta pr√°ctica.

## 1.1 El problema en este ejercicio

El desenlace *Y* de inter√©s para este ejercicio es la variable glucosa medida en ayunas. Veamos la distribuci√≥n de la variable y el promedio en en un histograma.

```{r}
circun_glucosa |>     ggplot(aes(x = glucosa_mg_dL)) +   geom_histogram(     color = "white",     ) +    labs(y = "Frecuencia",         x = "Glucosa (mg/dL)") +   geom_vline(xintercept = mean(circun_glucosa$glucosa_mg_dL, na.rm = TRUE),              color = "darkred", size = 1.5)
```

En estos datos, el promedio de la glucosa es:

```{r}
mean(circun_glucosa$glucosa_mg_dL, na.rm = TRUE)
```

Una observaci√≥n importante a partir del histograma y el promedio (el valor esperado) es que existe una gran variaci√≥n entre los valores de glucosa de los individuos de quienes provienen los datos. Podemos hipotetizar de que otras variables (predictores) podr√≠an influir en esta variaci√≥n, por ejemplo, la circunferencia de cintura.

## 1.2 Notaci√≥n en el m√©todo de regresi√≥n lineal simple

El m√©todo de regresi√≥n lineal simple encuentra la l√≠nea que mejor se ajusta a la descripci√≥n lineal entre la glucosa en ayunas y la circunferencia de la cintura, tal como se muestra en la siguiente figura:

```{r}
plot(glucosa_mg_dL ~ circunf_cintura_cm , data = circun_glucosa,      col = "gray",      ylab = "Glucosa (mg/dL)",      xlab = "Circunferencia de cintura (cm)",      las = 1,      pch = 20,       font.lab = 2, font.axis = 2)   # La funci√≥n lm() ajusta el modelo de regresi√≥n lineal abline(lm(glucosa_mg_dL ~ circunf_cintura_cm , data = circun_glucosa), lwd = 2, col = "darkred")
```

La ecuaci√≥n siguiente ecuaci√≥n describe un modelo de regresi√≥n lineal simple para ùëå usando un predictor continuo ùëã. $$ Y = \beta_0 + \beta_1 X + \epsilon $$ Cuando ajustamos un modelo de regresi√≥n lineal simple a nuestros datos, estimamos (hallamos) los par√°metros del modelo que mejor explican la relaci√≥n entre las dos variables (desenlace y predictor), incluyendo los coeficientes (Œ≤‚ÇÄ, Œ≤‚ÇÅ) y el error (ùúÄ), que representa la variabilidad no explicada por el modelo.

Para un predictor continuo, el intercepto (Œ≤‚ÇÄ) es el valor esperado de Y cuando X = 0 (es decir, el promedio del resultado cuando el predictor es cero). La pendiente (Œ≤‚ÇÅ) es el cambio promedio en Y por cada unidad de cambio en X. El t√©rmino de error (ùúÄ) representa la diferencia entre los valores observados y los valores predichos por el modelo.

Aplicado a nuestro ejemplo, el intercepto (Œ≤‚ÇÄ) representa la circunferencia de cintura promedio cuando la glucosa en ayunas es cero (aunque este valor puede no tener sentido pr√°ctico, es necesario matem√°ticamente). La pendiente (Œ≤‚ÇÅ) indica cu√°nto aumenta (o disminuye) en promedio la circunferencia de la cintura por cada unidad adicional de glucosa en ayunas (medida en mg/dL). El error (ùúÄ) recoge la variaci√≥n individual que no es explicada solo por la glucosa.

Asi que, como el objetivo es hallar los valores de los par√°metros (Œ≤‚ÇÄ,Œ≤‚ÇÅ,ùúÄ), es apropiado decir que estamos 'ajustando el modelo de regresi√≥n lineal simple' para el problema planteado (a.k.a la asociaci√≥n entre glucosa y la circunferencia de cintura)

## 1.3 Ajustando el modelo de regresi√≥n lineal simple para nuestro problema

En R, usamos la funci√≥n lm() para ajustar un modelo de regresi√≥n lineal. "lm" es la abreviatura para "linear model". Dentro de la funci√≥n debemos indicarle como argumentos el desenlace X, el predictor Y y la data donde se encuentran las variables. Esta es la estructura para ajustar el modelo con la funci√≥n lm: lm(y \~ x, data = mis_datos).

Ajustando el modelo para nuestros datos

```{r}
modelo_ejemplo = lm(glucosa_mg_dL ~ circunf_cintura_cm, data = circun_glucosa)
```

Para ver los resultados, usamos la funci√≥n summary() y dentro, el objeto modelo_ejemplo.

```{r}
summary(modelo_ejemplo)
```

## 1.4 Interpretando los resultados

La secci√≥n Coefficients del resultado:

```{r}
summary(modelo_ejemplo)$coef
```

...muestra las estimaciones y las pruebas de hip√≥tesis para el intercepto (Œ≤‚ÇÄ), etiquetado como (Intercept), y para el coeficiente de la circunferencia de cintura (la pendiente, Œ≤‚ÇÅ), etiquetado como Circunfe_brazo_cm.

En esta misma secci√≥n, la columna Estimate muestra los coeficientes estimados del modelo de regresi√≥n lineal simple. As√≠, el modelo que mejor se ajusta tiene un intercepto de 59.474 y una pendiente de 0.49970.

La tabla de coeficientes tambi√©n muestra el error est√°ndar de cada estimaci√≥n, su valor t y su valor p (etiquetado como Pr(\>\|t\|)). El valor p del intercepto usualmente no es de inter√©s, pero el valor p del predictor (Circunfe_brazo_cm) prueba la hip√≥tesis nula de que el desenlace NO tiene asociaci√≥n con el predictor o, dicho de otra manera, que la pendiente es cero. La hip√≥tesis nula plantea que la l√≠nea de mejor ajuste es una l√≠nea horizontal, lo que indicar√≠a que el promedio esperado del desenlace es el mismo en todos los valores del predictor; es decir, que no existe asociaci√≥n entre el desenlace (glucosa) y el predictor (circunferencia de cintura).

Finalmente, el valor R-cuadrado es una medida de bondad de ajuste que var√≠a entre 0 (sin asociaci√≥n) y 1 (asociaci√≥n lineal perfecta), y corresponde al cuadrado de la correlaci√≥n de Pearson entre el desenlace y el predictor. Se interpreta como la proporci√≥n de la variaci√≥n en el desenlace que es explicada por el modelo. En nuestro modelo, el R¬≤ (R-cuadrado) es 0.0871. Esto significa que aproximadamente el 8.6% de la variaci√≥n en los valores de glucosa en ayunas se explica por la circunferencia de la cintura

## 1.5 ¬øC√≥mo reportar los resultados del ajuste del modelo de regresi√≥n lineal simple?

Tanto si se trata de una tesis o un art√≠culo, abajo un ejemplo de c√≥mo reportar los resultados del presente problema:

> "(...) empleamos un modelo de regresi√≥n linear para evaluar la asociaci√≥n entre el nivel de glucosa en ayunas (mg/dL) y la circunferencia de cintura (cm) usando datos de 965 adultos. 8.71% de la variaci√≥n en el nivel de glucosa en ayunas fue explicada por la circunferencia de cintura (R¬≤=0.0871). Se encontr√≥ una asociaci√≥n positiva significativa entre la glucosa en ayunas y la circunferencia de cintura (B=0.499; p \<.001). En promedio, por cada diferencia de 1 cm en la circunferencia de cintura, los adultos difieren en el promedio de glucosa en ayunas en 0.499 mg/dL"

Adicionalmente, es buena idea presentar los resultados en un tabla.

```{r}
theme_gtsummary_language("es")

tabla_reporte <- modelo_ejemplo |> 
  tbl_regression(intercept = T,
                        estimate_fun = function(x) style_sigfig(x, digits = 4),
                        pvalue_fun   = function(x) style_pvalue(x, digits = 3),
                 label        = list(circunf_cintura_cm ~ "Circunferencia de cintura (cm)")) |>
  modify_caption("Regresi√≥n de la glucosa en ayunas (mg/dL) en funci√≥n de la circunferencia de cintura")

tabla_reporte
```

**Exportamos la tabla**

```{r}
tabla_reporte |>    as_flex_table()  |>    flextable::save_as_docx(path = "tabla_reporte.docx")
```

# 2 Prueba t de Student para muestras independientes

Imagina que, ahora, luego de haber tomado las mediciones de medidas de glucosa en ayunas (mg/dL) queremos saber si el promedio de glucosa en varones es significativamente diferente del promedio de glucosa en mujeres. Es esta situaci√≥n, hay dos grupos (varones y mujeres) de muestras independientes.

## 2.1 ¬øCu√°ndo usar la prueba t de Student para muestras independientes?

-   Cuando los dos grupos de muestras a comparar han sido muestreadas de una distribuci√≥n normal. Aqu√≠ podemos usar la prueba de Shapiro-Wilk.

-   Cuando las varianzas de los dos grupos son iguales. Esto puede ser evaluado con la prueba de Levene o la prueba F.

2.2 Usualmente, la hip√≥tesis de la prueba t de Student son:

-   Hip√≥tesis nula (H‚ÇÄ): No hay diferencia entre las medias de los dos grupos. $$ H_0: \mu_1 = \mu_2 $$
-   Hip√≥tesis alternativa (H‚ÇÅ): Hay una diferencia entre las medias de los dos grupos. $$ H_1: \mu_1 \neq \mu_2 $$

## 2.2 Sobre los datos para esta pr√°ctica

El dataset circun_glucosa, de 1000 personas adultas (\>=20 a√±os de edad), contiene datos circunferencia de cintura (en cent√≠metros), la variable sexo y otros datos demogr√°ficos.

## 2.3 Resumen y visualizaci√≥n

Resumen

Antes de realizar la prueba t de Student es importante conocer la distribuci√≥n de los datos e identificar si hay valores perdidos o at√≠picos. Empecemos por el resumen:

```{r}
group_by(circun_glucosa, sexo) |> 
  summarise(
    count = n(),
    mean = mean(circunf_brazo_cm, na.rm = TRUE),
    sd = sd(circunf_brazo_cm, na.rm = TRUE)
  )
```

Visualizaci√≥n

```{r}
circun_glucosa |>  
  filter(!is.na(sexo)& !is.na(circunf_brazo_cm))  |> 
  ggplot(aes(y = circunf_brazo_cm, x = sexo)) +
  geom_boxplot() +
  labs(y = "Circunferencia del brazo (cm)", x = "sexo")
```

## 2.4 Pruebas preliminares para evaluar los supuestos de la prueba t de Student

Supuesto 1: los datos deben haber sido muestreados de una distribuci√≥n normal.

Para esto, usamos la prueba de Shapiro-wilk.

```{r}
circun_glucosa |>    filter(sexo == "Masculino") |>    summarise(shapiro = list(shapiro.test(circunf_brazo_cm))) |>    pull(shapiro)
```

```{r}
circun_glucosa |>    filter(sexo == "Femenino") |>    summarise(shapiro = list(shapiro.test(circunf_brazo_cm))) |>    pull(shapiro)
```

Supuesto 2: Las varianzas de los dos grupos son iguales Para esto podemos usar la prueba F para evaluar la homogeneidad de varianzas. Esto esta implementado en la funci√≥n var.test()

```{r}
ls()

```

```         
```

```{r}
var.test(circunf_brazo_cm_sim ~ sexo, data = data_mod)
```

El valor p de la prueba F es p = 0.3143. Es mayor que el nivel de significancia Œ± = 0.05. En conclusi√≥n, no hay una diferencia significativa entre las varianzas de los dos conjuntos (femenino y masculino) de datos. Por lo tanto, podemos usar la prueba t cl√°sica que asume igualdad de varianzas.

## 2.5 Realizamos la prueba t para nuestros datos.

```{r}
t.test(circunf_brazo_cm ~ sexo, data = circun_glucosa, var.equal = TRUE)
```

**Interpretando los resultados**

El valor p de la prueba es 0.003615, lo cual es menor que el nivel de significancia Œ± = 0.05. Por lo tanto, podemos concluir que la circunferencia promedio del brazo en hombres es significativamente diferente de la circunferencia promedio en mujeres.

PC3-1\
SEMANA 10

# Cargamos e instalamos paquetes

```{r}
install.packages("car")
install.packages("cards")
install.packages("broom.helpers")
```

```{r}
library(tidyverse)
library(here)
library(rio)
library(gtsummary)
library(car)
library(cards)
library(broom.helpers)
```

## Cargando los datos

```{r}
hipert_covid <- import(here("data", "s10_hipert_covid.csv"))
```

```{r}
asma <- import(here("data", "s10_asma.csv"))
```

## 1.2 Estimando OR usando regresi√≥n log√≠stica para un predictor categ√≥rico

```{r}
hipert_covid_1 <- hipert_covid |> 
  mutate(hipert = relevel(as.factor(hipert), ref = "no"),
         desenlace = relevel(as.factor(desenlace), ref = "vivo"))
```

A continuaci√≥n, usamos la funci√≥n `glm()`, general linear model, con el argumento family = binomial para ajustar una regresi√≥n log√≠stica y `summary()` para ver los resultados.

```{r}
regre_log <- glm(desenlace ~ hipert,
                 family = binomial, 
                 data = hipert_covid_1)

summary(regre_log)
```

Para obtener el OR en s√≠ (como usualmente se reporta en los estudios), exponenciamos el coeficiente usando la funci√≥n exp()

```{r}
exp(coef(regre_log)[-1]) # [-1] elimina la primera fila, al intercepto.
```

Usamos la funci√≥n `confint()` para calcular los intervalos de confianza (IC) al 95% para el coeficientes de regresi√≥n, y exponenciamos estos valores para obtener los IC del 95% para los OR.

```{r}
exp(confint(regre_log))[-1, , drop=F]
```

## 1.4 Estimando OR usando regresi√≥n log√≠stica para un predictor num√©rico

```{r}
regre_log_1 <- glm(desenlace ~ edad, family = binomial, data = hipert_covid_1)

summary(regre_log_1)$coef
```

```{r}
exp(coef(regre_log_1)[-1])
```

```{r}
exp(confint(regre_log_1)[-1,])
```

```{r}
theme_gtsummary_language(language = "es")
```

```{r}
tabla_reg_logi <- hipert_covid_1 |>
  tbl_uvregression(
    include = c(edad, sexo, hipert),
    y = desenlace,
    method = glm,
    method.args = list(family = binomial),
    exponentiate = TRUE,
    conf.int = TRUE,
    hide_n = TRUE,
    add_estimate_to_reference_rows = FALSE,
    pvalue_fun = ~ style_pvalue(.x, digits = 3),
    estimate_fun = ~ style_number(.x, digits = 2),
    label = list(
      edad ~ "Edad (a√±os)",
      sexo ~ "Sexo",
      hipert ~ "Hipertensi√≥n"
    )
  ) |>
  bold_labels() |>
  bold_p(t = 0.05) |>
  modify_header(estimate = "**OR no ajustado**", p.value = "**Valor P**")
```

IMPRIMIMOS LAS TABLAS

```{r}
tabla_reg_logi
```

## 2.3 Ajustamos modelos de regresi√≥n de Poisson

```{r}
reg_poisson1 = glm(episod_asma ~ sexo, data = asma, family = "poisson")
summary(reg_poisson1)
```

```{r}
reg_poisson2 = glm(episod_asma ~ infec_resp_recur, data = asma, family = "poisson")
summary(reg_poisson2)
```

```{r}
reg_poisson2 = glm(episod_asma ~ ghq12, data = asma, family = "poisson")
summary(reg_poisson2)
```

## 2.4 C√≥mo interpretar y reportar los resultados de una regresi√≥n de Poisson

```{r}
tabla_reg_poisson <- asma |>
  tbl_uvregression(
    include = c(sexo, infec_resp_recur, ghq12),
    y = episod_asma,
    method = glm,
    method.args = list(family = poisson),
    exponentiate = TRUE,
    conf.int = TRUE,
    hide_n = TRUE,
    add_estimate_to_reference_rows = FALSE,
    pvalue_fun = ~ style_pvalue(.x, digits = 3),
    estimate_fun = ~ style_number(.x, digits = 2),
    label = list(
      sexo ~ "Sexo",
      infec_resp_recur ~ "Infecci√≥n respiratoria recurrente",
      ghq12 ~ "Bienestar psicol√≥gico"
    )
  ) |>
  bold_labels() |>
  bold_p(t = 0.05) |>
  modify_header(estimate = "**IRR no ajustado**", p.value = "**Valor P**")
```

```{r}
tabla_reg_poisson
```

Bas√°ndonos en esta tabla, podemos interpretar los resultados de la siguiente manera:

Ser del sexo femenino esta asociado a un menor riesgo de sufrir un ataque asm√°tico, con un IRR de 0.74 (IC 95%: 0.58, 0.94).

Aquellos con infecci√≥n respiratoria recurrente tienen un mayor riesgo de sufrir un ataque asm√°tico, con un IRR de 2.47 (IC 95%: 1.84, 3.26).

Un aumento de un punto en la puntuaci√≥n GHQ-12 (que evalua el bienestar psicol√≥gico) incrementa el riesgo de tener un ataque asm√°tico en 1.06 (IC 95%: 1.05, 1.06).

## Instalar y cargar los paquetes

```{r}
install.packages("factoextra")
install.packages("cluster")
```

```{r}
library(factoextra)
library(cluster)
library(here)
library(rio)
library(tidyverse)
```

# 1 ¬øC√≥mo aplicaremos Machine Learning a esta sesi√≥n?

Para intentar responder preguntas de investigaci√≥n a veces es necesario que se realicen muchas medidas en una misma muestra. Por ejemplo, adem√°s de recolectar variables usuales como la edad, sexo y comobilidades, podr√≠amos recolectar tambien varios otros par√°metros laboratoriales como creatinina s√©rica, glucosa, hemoglobina glicosilada, y varios otros adicionales. Y lo cierto es que es posible que existan patrones entre los valores de las variables. Es decir, es posible que haya una dependencia entre las variables predictoras. Por ejemplo, si un grupo de pacientes tienen insuficiencia renal aguda, algunos par√°metros renales de laboratorio tendr√°n valores fuera del rango normal, mientras que otros par√°metros, no. Un opci√≥n para aplicar t√©cnicas convencionales es la excluir variables redundantes o variables que podamos encontrar como "no interesantes". No obstante, esto puede llevar a p√©rdida de informaci√≥n. Para estas situaciones se pueden usar t√©cnicas de machine learning como las t√©cnicas de agrupamiento (clustering), la cual permitan la inclusi√≥n de multiple variables y permite definir grupos de pacientes que comparten similitudes respecto a las variables inclu√≠das.

## 1.1 Uso de las t√©cnicas de agrupamiento para responden preguntas de investigaci√≥n en salud

Las t√©cnicas de agrupamiento son un tipo de t√©cnica exploratoria que puede usarse con el objetivo de clasificar observaciones (por ejemplo pacientes que forman parte de una muestra) en grupos en base a su similaridad y desimilaridad de las variables. A partir de esto, obtendremos grupos cuyos individuos que pertenecen a un mismo grupo son similares pero diferentes a individuos que pertenecen a otros grupos.

Los grupos encontrados pueden ser usados para hacer predicciones o evaluar diferencias en par√°metros de laboratorio. Por ejemplo, entre grupos encontrados de pacientes quienes iniciaron su tratamiento para el c√°ncer, podemos comparar su supervivencia, calidad de vida luego de dos a√±os u otras medidas a partir de los clusters (grupos) encontrados.

# 2 An√°lisis de agrupamiento herarquico (Hierarchical Clustering)

## 2.1 Sobre el problema para esta sesi√≥n

El dataset de esta sesi√≥n contiene informaci√≥n de 160 pacientes que han iniciado tratamiento de hemodi√°lisis en un hospital p√∫blico de Lima, Per√∫. El dataset incluye variables num√©ricas de laboratorio que eval√∫an distintos perfiles cl√≠nicos, como el renal, hep√°tico, electrol√≠tico, lip√≠dico, entre otros. El objetivo de este ejercicio es aplicar el m√©todo de agrupamiento jer√°rquico para identificar grupos de pacientes que compartan caracter√≠sticas similares en cuanto a su estado de salud basal, lo que permitir√° proponer posibles categor√≠as de riesgo o patrones cl√≠nicos diferenciados.

## 2.2 El dataset para esta sesi√≥n

Para ilustrar el proceso de an√°lisis usaremos el dataset llamado `hemo_data` el cual contiene 160 observaciones con las siguientes variables: edad (a√±os), sexo (masculino/femenino), enfermedad renal de base (variable categ√≥rica), peso corporal (kilogramos), talla (cent√≠metros), √≠ndice de masa corporal (IMC, kilogramos/metro cuadrado), presi√≥n arterial sist√≥lica (mil√≠metros de mercurio, mmHg), presi√≥n arterial diast√≥lica (mmHg), volumen urinario (mililitros por d√≠a), hemoglobina (gramos por decilitro, g/dL), recuento de leucocitos (miles por microlitro, 10¬≥/ŒºL), recuento de plaquetas (miles por microlitro, 10¬≥/ŒºL), prote√≠nas totales (g/dL), alb√∫mina (g/dL), aspartato aminotransferasa (AST, unidades por litro, U/L), alanina aminotransferasa (ALT, U/L), gamma-glutamil transferasa (Œ≥-GTP, U/L), fosfatasa alcalina (U/L), √°cido √∫rico s√©rico (miligramos por decilitro, mg/dL), nitr√≥geno ureico en sangre (BUN, mg/dL), creatinina s√©rica (mg/dL), sodio s√©rico (miliequivalentes por litro, mEq/L), potasio s√©rico (mEq/L), cloruro s√©rico (mEq/L), calcio s√©rico (mg/dL), f√≥sforo s√©rico (mg/dL), magnesio s√©rico (mg/dL), prote√≠na C-reactiva (mg/dL), colesterol total (mg/dL), triglic√©ridos (mg/dL), colesterol de lipoprote√≠nas de alta densidad (HDL, mg/dL), hierro s√©rico (microgramos por decilitro, Œºg/dL), ferritina s√©rica (nanogramos por mililitro, ng/mL), capacidad insaturada de fijaci√≥n de hierro (UIBC, Œºg/dL), p√©ptido natriur√©tico tipo B (BNP, picogramos por mililitro, pg/mL), hormona paratiroidea intacta (iPTH, pg/mL), glucosa s√©rica (mg/dL), hemoglobina glicosilada (HbA1c, porcentaje), Œ≤2-microglobulina (miligramos por litro, mg/L), gravedad espec√≠fica urinaria (sin unidad), pH urinario (sin unidad), sodio urinario (mEq/L), potasio urinario (mEq/L), cloruro urinario (mEq/L), calcio urinario (mg/dL), f√≥sforo urinario (mg/dL), magnesio urinario (mg/dL), nitr√≥geno ureico urinario (mg/dL), creatinina urinaria (mg/dL), √°cido √∫rico urinario (mg/dL), prote√≠na urinaria (mg/dL), N-acetil-Œ≤-D-glucosaminidasa (NAG, U/L), Œ±1-microglobulina urinaria (mg/L) y prote√≠na de uni√≥n a √°cidos grasos en h√≠gado tipo L (L-FABP, ng/mL)

### 2.2.1 Importando los datos

```{r}
hemo_data <- import(here("data", "s13_hemodialisis.csv"))
```

## 2.3 Preparaci√≥n de los datos

### 2.3.1 Solo datos num√©ricos

Para el an√°lisis de agrupamiento jer√°rquico de esta sesi√≥n usaremos solo variables num√©ricas. Es posible emplear variables categ√≥ricas en esta t√©cnica, pero esto no ser√° cubierto aqu√≠. El c√≥digo abajo elimina las variables categ√≥ricas `Sexo` y `Enfermedad_renal`. `id` ser√° el identificador para los participantes.

```{r}
hemo_data_1 = hemo_data |> 
  select(-Sexo, -Enfermedad_Renal) |> 
  column_to_rownames("id")
```

### 2.3.2 La importancia de estandarizar

Adicionalmente, es fundamental estandarizar las variables antes de realizar el an√°lisis de agrupamiento jer√°rquico. Estandarizar significa transformar las variables a una escala com√∫n para hacerlas comparables entre s√≠. Esto es especialmente importante porque uno de los pasos clave en el m√©todo de agrupamiento consiste en calcular distancias entre los objetos (en este caso, los pacientes) a partir de las variables cl√≠nicas incluidas en el dataset. Sin embargo, dichas variables se encuentran originalmente medidas en diferentes escalas y unidades. Por ejemplo, el √≠ndice de masa corporal (IMC) se expresa en kilogramos por metro cuadrado (kg/m¬≤), mientras que la creatinina s√©rica se mide en miligramos por decilitro (mg/dL). Si no se realiza una estandarizaci√≥n previa, las variables con valores num√©ricos m√°s grandes o con unidades distintas podr√≠an influir desproporcionadamente en el c√°lculo de distancias, generando agrupamientos sesgados o poco representativos de la verdadera estructura de los datos.

Para ilustrar este punto: si se agrupa a los pacientes considerando simult√°neamente su IMC (kg/m¬≤) y su nivel de creatinina s√©rica (mg/dL), cabe preguntarse: ¬øuna diferencia de 1 kg/m¬≤ en IMC es tan relevante como una diferencia de 1 mg/dL en creatinina? ¬øQu√© variable deber√≠a tener mayor peso en la formaci√≥n de los grupos? Sin una estandarizaci√≥n previa, estas diferencias no ser√≠an comparables, y las variables con mayor rango num√©rico dominar√≠an el c√°lculo de distancias, afectando los resultados de la clasificaci√≥n. Por ello, es imprescindible aplicar una funci√≥n de estandarizaci√≥n, como `scale()` en R, que transforma las variables para que tengan media cero y desviaci√≥n est√°ndar uno, permitiendo as√≠ que todas contribuyan equitativamente al an√°lisis.

```{r}
hemo_data_escalado = scale(hemo_data_1)
```

Un vistazo a los datos antes del escalamiento:

```{r}
head(hemo_data_1)
```

y un vistazo despu√©s del escalamiento:

```{r}
head(hemo_data_escalado)
```

## 2.4 C√°lculo de distancias

Dado que uno de los pasos es encontrar "cosas similares", necesitamos definir "similar" en t√©rminos de distancia. Esta distancia la calcularemos para cada par posible de objetos (participantes) en nuestro dataset. Por ejemplo, si tuvieramos a los pacientes A, B y C, las distancia se calcular√≠an para A vs B; A vs C; y B vs C. En R, podemos utilizar la funci√≥n `dist()` para calcular la distancia entre cada par de objetos en un conjunto de datos. El resultado de este c√°lculo se conoce como matriz de distancias o de disimilitud.

```{r}
dist_hemo_data <- dist(hemo_data_escalado, method = "euclidean")
```

## 2.4.1 (opcional) Visualizando las distancias euclidianas con un mapa de calor

Una forma de visualizar si existen patrones de agrupamiento es usando mapas de calor (heatmaps). En R usamos la funci√≥n `fviz_dist()` del paquete factoextra para crear un mapa de calor.

```{r}
fviz_dist(dist_hemo_data)
```

El nivel del color en este gr√°fico, es proporcional al valor de disimilaridad en observaciones (pacientes). Ejemplo, un color rojo puro indica una distancia con valor de 0 entre las abservaciones. Nota que la l√≠nea diagonal corresponde al intercepto de las mismas observaciones. Las observaciones que pertenecen a un mismo cluster (grupo) caen en orden consecutivo. Una conclusi√≥n del gr√°fico de abajo es que hay grupos que comparten similaridades dado que observamos grupos de colores.

## 2.5 El m√©todo de agrupamiento: funci√≥n de enlace (linkage)

El agrupamiento jer√°rquico es un m√©todo que empieza agrupando las observaciones m√°s parecidas entre s√≠, por lo que es f√°cil de usar al comienzo. Sin embargo, no basta con calcular las distancias entre todos los pares de objetos. Una vez que se forma un nuevo grupo (cl√∫ster), hay que decidir c√≥mo medir la distancia entre ese grupo y los dem√°s puntos o grupos ya existentes. Hay varias formas de hacerlo, y cada una genera un tipo diferente de agrupamiento jer√°rquico. La funci√≥n de enlace (linkage) toma la informaci√≥n de distancias devuelta por la funci√≥n `dist()` y agrupa pares de objetos en cl√∫steres bas√°ndose en su similitud. Luego, estos nuevos cl√∫steres formados se enlazan entre s√≠ para crear cl√∫steres m√°s grandes. Este proceso se repite hasta que todos los objetos del conjunto de datos quedan agrupados en un √∫nico √°rbol jer√°rquico. Hay varios m√©todos para realizar este agrupamiento, incluyendo *Enlace m√°ximo o completo*, *Enlace m√≠nimo o simple*, *Enlace de la media o promedio*, *Enlace de centroide*, *M√©todo de varianza m√≠nima de Ward*. No entraremos en detalle sobre c√≥mo funciona estos m√©todos, pero para este contexto el m√©todo de varianza minima de Ward o el m√©todo m√°ximo, son preferidos. En este ejemplo, usamos el m√©todo de varianza m√≠nima de Ward.

```{r}
dist_link_hemo_data <- hclust(d = dist_hemo_data, method = "ward.D2")
```

## 2.7 Dendrogramas para la visualizaci√≥n de patrones

Los dendrogramas es una representaci√≥n gr√°fica del √°rbol jer√°rquico generado por la funci√≥n `hclust()`.

```{r}
fviz_dend(dist_link_hemo_data, cex = 0.7)
```

Un dendrograma es como un √°rbol geneal√≥gico para los cl√∫steres (grupos). Esta muestra c√≥mo los puntos de datos individuales o los grupos de datos se van uniendo entre s√≠. En la parte inferior, cada punto de datos se representa como un grupo independiente, y a medida que se asciende, los grupos similares se combinan. Cuanto m√°s bajo es el punto de uni√≥n, mayor es la similitud entre los grupos.

## 2.8 ¬øC√∫antos grupos se formaron en el dendrograma?

Uno de los problemas con la agrupaci√≥n jer√°rquica es que no nos dice cu√°ntos grupos hay ni d√≥nde cortar el dendrograma para formar grupos. Aqu√≠ entra en juego la decisi√≥n del investigador a partir de analizar el dendrograma. Para nuestro dendrograma, es claro que el dendrograma muestra tres grupos. En el c√≥digo de abajo, el argumento k = 3 define el n√∫mero de clusters.

```{r}
fviz_dend(dist_link_hemo_data, 
          k = 3,
          cex = 0.5,
          k_colors = c("#2E9FDF", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, 
          rect = TRUE)
```

# 3 Agrupamiento con el algoritmo K-Means

El m√©todo de agrupamiento (usando el algoritmo) K-means es la t√©cnica de machine learning m√°s utilizado para dividir un conjunto de datos en un n√∫mero determinado de k grupos (es decir, k cl√∫steres), donde k representa el n√∫mero de grupos predefinido por el investigador. Esto contrasta con la t√©cnica anterior, dado que aqu√≠ s√≠ iniciamos con un grupo pre-definido cuya idoniedad (de los grupos) puede ser evaluado. En detalle, el esta t√©cnica clasifica a los objetos (participantes) del dataset en m√∫ltiples grupos, de manera que los objetos dentro de un mismo cl√∫ster sean lo m√°s similares posible entre s√≠ (alta similitud intragrupo), mientras que los objetos de diferentes cl√∫steres sean lo m√°s diferentes posible entre ellos (baja similitud intergrupo). En el agrupamiento k-means, cada cl√∫ster se representa por su centro (centroide), que corresponde al promedio de los puntos asignados a dicho cl√∫ster.

Aqu√≠ como funciona el algoritmo de K-Means

1.  Indicar cu√°ntos grupos (cl√∫steres) se quieren formar. Por ejemplo, si se desea dividir a los pacientes en 3 grupos seg√∫n sus caracter√≠sticas cl√≠nicas, entonces K=3.
2.  Elegir aleatoriamente K casos del conjunto de datos como centros iniciales. Por ejemplo, R selecciona al azar 3 pacientes cuyas caracter√≠sticas (edad, IMC, creatinina, etc.) servir√°n como punto de partida para definir los grupos.
3.  Asignar cada paciente al grupo cuyo centro est√© m√°s cerca, usando la distancia euclidiana. Es como medir con una regla cu√°l centroide (paciente promedio) est√° m√°s pr√≥ximo a cada paciente en funci√≥n de todas sus variables.
4.  Calcular un nuevo centro para cada grupo. Es decir, calcular el promedio de todas las variables de los pacientes que quedaron en ese grupo. Por ejemplo, si en el grupo 1 quedaron 40 pacientes, el nuevo centroide ser√° el promedio de la edad, IMC, creatinina, etc., de esos 40 pacientes. Este centroide es un conjunto de valores (uno por cada variable).
5.  Repetir los pasos 3 y 4 hasta que los pacientes dejen de cambiar de grupo o hasta alcanzar un n√∫mero m√°ximo de repeticiones (en R, por defecto son 10 repeticiones). Esto permitir√° que los grupos finales sean estables.

## 3.1 El problema y dataset para este ejercicio

Usaremos el mismo dataset y el mismo problema que el que empleamos en el ejercicio anterior (para Agrupamiento Jer√°rquico).

## 3.2 Estimando el n√∫mero √≥ptimo de clusters

Como indiqu√© arriba, el m√©todo de agrupamiento k-means requiere que el usuario especifique el n√∫mero de cl√∫steres (grupos) a generar. Una pregunta fundamental es: ¬øc√≥mo elegir el n√∫mero adecuado de cl√∫steres esperados (k)?

Aqu√≠ muestro una soluci√≥n sencilla y popular: realizar el agrupamiento k-means probando diferentes valores de k (n√∫mero de cl√∫steres). Luego, se grafica la suma de cuadrados dentro de los cl√∫steres (WSS) en funci√≥n del n√∫mero de cl√∫steres. En R, podemos usar la funci√≥n fviz_nbclust() para estimar el n√∫mero √≥ptimo de cl√∫steres.

# 3 Agrupamiento con el algoritmo K-Means

El m√©todo de agrupamiento (usando el algoritmo) K-means es la t√©cnica de machine learning m√°s utilizado para dividir un conjunto de datos en un n√∫mero determinado de k grupos (es decir, k cl√∫steres), donde k representa el n√∫mero de grupos predefinido por el investigador. Esto contrasta con la t√©cnica anterior, dado que aqu√≠ s√≠ iniciamos con un grupo pre-definido cuya idoniedad (de los grupos) puede ser evaluado. En detalle, el esta t√©cnica clasifica a los objetos (participantes) del dataset en m√∫ltiples grupos, de manera que los objetos dentro de un mismo cl√∫ster sean lo m√°s similares posible entre s√≠ (alta similitud intragrupo), mientras que los objetos de diferentes cl√∫steres sean lo m√°s diferentes posible entre ellos (baja similitud intergrupo). En el agrupamiento k-means, cada cl√∫ster se representa por su centro (centroide), que corresponde al promedio de los puntos asignados a dicho cl√∫ster.

Aqu√≠ como funciona el algoritmo de K-Means

1.  Indicar cu√°ntos grupos (cl√∫steres) se quieren formar. Por ejemplo, si se desea dividir a los pacientes en 3 grupos seg√∫n sus caracter√≠sticas cl√≠nicas, entonces K=3.
2.  Elegir aleatoriamente K casos del conjunto de datos como centros iniciales. Por ejemplo, R selecciona al azar 3 pacientes cuyas caracter√≠sticas (edad, IMC, creatinina, etc.) servir√°n como punto de partida para definir los grupos.
3.  Asignar cada paciente al grupo cuyo centro est√© m√°s cerca, usando la distancia euclidiana. Es como medir con una regla cu√°l centroide (paciente promedio) est√° m√°s pr√≥ximo a cada paciente en funci√≥n de todas sus variables.
4.  Calcular un nuevo centro para cada grupo. Es decir, calcular el promedio de todas las variables de los pacientes que quedaron en ese grupo. Por ejemplo, si en el grupo 1 quedaron 40 pacientes, el nuevo centroide ser√° el promedio de la edad, IMC, creatinina, etc., de esos 40 pacientes. Este centroide es un conjunto de valores (uno por cada variable).
5.  Repetir los pasos 3 y 4 hasta que los pacientes dejen de cambiar de grupo o hasta alcanzar un n√∫mero m√°ximo de repeticiones (en R, por defecto son 10 repeticiones). Esto permitir√° que los grupos finales sean estables.

## 3.1 El problema y dataset para este ejercicio

Usaremos el mismo dataset y el mismo problema que el que empleamos en el ejercicio anterior (para Agrupamiento Jer√°rquico).

## 3.2 Estimando el n√∫mero √≥ptimo de clusters

Como indiqu√© arriba, el m√©todo de agrupamiento k-means requiere que el usuario especifique el n√∫mero de cl√∫steres (grupos) a generar. Una pregunta fundamental es: ¬øc√≥mo elegir el n√∫mero adecuado de cl√∫steres esperados (k)?

Aqu√≠ muestro una soluci√≥n sencilla y popular: realizar el agrupamiento k-means probando diferentes valores de k (n√∫mero de cl√∫steres). Luego, se grafica la suma de cuadrados dentro de los cl√∫steres (WSS) en funci√≥n del n√∫mero de cl√∫steres. En R, podemos usar la funci√≥n fviz_nbclust() para estimar el n√∫mero √≥ptimo de cl√∫steres.

Primero escalamos los datos:

```{r}
hemo_data_escalado = scale(hemo_data_1)
```

Ahora graficamos la suma de cuadrados dentro de los gr√°ficos

```{r}
fviz_nbclust(hemo_data_escalado, kmeans, nstart = 25, method = "wss") + 
  geom_vline(xintercept = 3, linetype = 2)
```

El punto donde la curva forma una "rodilla" o quiebre suele indicar el n√∫mero √≥ptimo de cl√∫steres. Para nuestro gr√°fico, es en el n√∫mero de cluster 3.

## 3.3 C√°lculo del agrupamiento k-means

Dado que el resultado final del agrupamiento k-means es sensible a las asignaciones aleatorias iniciales, se especifica el argumento `nstart = 25`. Esto significa que R intentar√° 25 asignaciones aleatorias diferentes y seleccionar√° la mejor soluci√≥n, es decir, aquella con la menor variaci√≥n dentro de los cl√∫steres. El valor predeterminado de `nstart` en R es 1. Sin embargo, se recomienda ampliamente utilizar un valor alto, como 25 o 50, para obtener un resultado m√°s estable y confiable. El valor empleado aqu√≠, fue usado para determinar el n√∫mero de cl√∫steres √≥ptimos.

```{r}
set.seed(123)
km_res <- kmeans(hemo_data_escalado, 3, nstart = 25)
```

```{r}
km_res
```

El resultado muestra dos cosas:

1.  **Las medias o centros de los cl√∫steres** (*Cluster means*): una matriz cuyas filas corresponden al n√∫mero de cl√∫ster (1 a 3) y cuyas columnas representan las variables.

2.  **Un vector de asignaci√≥n de cl√∫ster** (*Clustering vector*): un vector de n√∫meros enteros (de 1 a 3) que indica a qu√© cl√∫ster ha sido asignado cada punto (para nuestro dataset, cada paciente).

## 3.4 Visualizaci√≥n de los cl√∫steres k-means

Al igual que el an√°lisis anterior, los datos se pueden representar en un gr√°fico de dispersi√≥n, coloreando cada observaci√≥n o paciente seg√∫n el cl√∫ster al que pertenece. El problema es que los datos contienen m√°s de dos variables, y surge la pregunta de qu√© variables elegir para representar en los ejes X e Y del gr√°fico. Una soluci√≥n es reducir la cantidad de dimensiones aplicando un algoritmo de reducci√≥n de dimensiones, como el An√°lisis de Componentes Principales (PCA). El PCA transforma las 52 variables originales en dos nuevas variables (componentes principales) que pueden usarse para construir el gr√°fico.

La funci√≥n `fviz_cluster()` del paquete factoextra se puede usar para visualizar los cl√∫steres generados por k-means. Esta funci√≥n toma como argumentos los resultados del k-means y los datos originales (hemo_data_escalado).

```{r}
fviz_cluster
  km_res
  data = hemo_data_escalado
  palette = c("#2E9FDF", "#E7B800", "#FC4E07")
  ellipse.type = "euclid"
  repel = TRUE
  ggtheme = theme_minimal()
```

### 3.4.1 ¬øC√≥mo interpretar?

En el gr√°fico resultante, los participantes (las observaciones) se representan como puntos. La t√©cnica ha "creado" dimensiones, de las cuales dos de las m√°s importantes de estas son consideradas en el gr√°fico. El uso aqu√≠ del PCA es poder clasificar diferentes "cosas" distintas en grupos, por ejemplo pacientes que iniciaron hemodialisis y que tienen distintos niveles de par√°metros laboratoriales, de una manera que genere el menor error posible (en t√©rminos de predecir correctamente el tipo de c√©lula). Adem√°s de los tres grupos formados (bien formados), nuestro gr√°fico aqu√≠ y en el agrupamiento jer√°rquico no nos dice m√°s. Es necesario realizar an√°lisis adicionales para evaluar la utilidad de estos cl√∫steres, como por ejemplo, evaluar si la supervivencia entre estos tres grupos varia. O evaluar como, en promedio, var√≠an par√°metros importantes de laboratorio (por ejemplo creatinina y urea para evaluar la funci√≥n renal).

########### 

**Aviso sobre el dataset de esta sesi√≥n:** A diferencia de sesiones anteriores, el conjunto de datos empleado en esta sesi√≥n es completamente simulado y no corresponde a informaci√≥n real de pacientes ni a datos provenientes de alg√∫n repositorio en l√≠nea. Es importante tener en cuenta que, en conjuntos de datos reales, los grupos formados mediante el an√°lisis de agrupamiento pueden no ser tan claramente distinguibles como en estos ejemplos.
